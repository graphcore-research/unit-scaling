

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1. User guide &mdash; unit-scaling  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="shortcut icon" href="_static/scales.png"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. Limitations" href="limitations.html" />
    <link rel="prev" title="Unit Scaling" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            unit-scaling
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">1.1. Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-unit-scaling">1.2. What is unit scaling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-unit-scale-a-model">1.3. How to unit-scale a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-considerations-for-unit-scaling">1.4. Key considerations for unit scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimising-unit-scaled-models">1.5. Optimising unit-scaled models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">2. Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">3. API reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">unit-scaling</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">1. </span>User guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/user_guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="user-guide">
<h1><span class="section-number">1. </span>User guide<a class="headerlink" href="#user-guide" title="Link to this heading"></a></h1>
<p>We recommend that new users read this guide before attempting to implement a unit-scaled
model.</p>
<p>It covers a brief overview of the technique, a practical guide to using the
library, some key considerations when applying unit scaling, and a discussion of
optimising unit-scaled models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The library is currently in its <em>beta</em> release.
Some features have yet to be implemented and occasional bugs may be present.
We’re keen to help users with any problems they encounter.</p>
</div>
<section id="installation">
<h2><span class="section-number">1.1. </span>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<p>To install the <code class="code docutils literal notranslate"><span class="pre">unit-scaling</span></code> library, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">unit</span><span class="o">-</span><span class="n">scaling</span>
</pre></div>
</div>
<p>For those who wish to develop on the <code class="code docutils literal notranslate"><span class="pre">unit-scaling</span></code> codebase, clone or fork our
<a class="reference external" href="https://github.com/graphcore-research/unit-scaling.git">GitHub repo</a> and follow the
instructions in our <span class="xref std std-doc">developer guide</span>.</p>
</section>
<section id="what-is-unit-scaling">
<h2><span class="section-number">1.2. </span>What is unit scaling?<a class="headerlink" href="#what-is-unit-scaling" title="Link to this heading"></a></h2>
<p>Unit scaling is a paradigm for designing deep learning models that aims to scale all
tensors (weights, activations and gradients) so that their standard deviation is
approximately 1 for the first pass of model training, before any weight updates have
taken place. This can enable the use of low-precision number formats out-of-the-box.</p>
<p>“Scaling” simply involves multiplying the output of an operation by a scalar value.
We use the term “scale” to refer to the standard deviation of a tensor.
Many operations used in deep learning change the scale of their inputs, and often in
an arbitrary way. This library is a re-implementation of common
PyTorch ops, adding scaling factors to ensure that input scale is now preserved.</p>
<p>As unit scaling is a technique for training models (usually from scratch), it considers
the scaling of operations in the backward pass as well as the forward pass.
The scaling factors used here are all pre-determined, based on
assumptions regarding the distribution of input tensors (typically, that they are
normally-distributed and unit-scaled).</p>
<p>The advantage of using a unit-scaled model is as follows:</p>
<ol class="arabic simple">
<li><p>A standard deviation of 1 is a great starting-point from the perspective of
floating-point number formats. It gives roughly equal headroom for the scale to grow
or shrink during training before over/underflow occur.</p></li>
<li><p>Because of this, loss scaling is not required for unit-scaled models.
Although scales will drift from their unit starting-point during training,
scales have stayed within range for all unit-scaled models tested thus far.</p></li>
<li><p>This can enable the use of smaller, more efficient number formats out-of-the-box,
such as FP16 and even FP8.</p></li>
</ol>
</section>
<section id="how-to-unit-scale-a-model">
<h2><span class="section-number">1.3. </span>How to unit-scale a model<a class="headerlink" href="#how-to-unit-scale-a-model" title="Link to this heading"></a></h2>
<p>We recommend the following approach to applying unit scaling to a model. We assume here
that you have an existing PyTorch model which you wish to adapt to be unit-scaled,
though a similar approach can be used to design a unit-scaled model from scratch.</p>
<p><strong>1. Consider your number formats</strong></p>
<p>The key motivation for unit scaling is to help keep values in the range of their number
formats. Given this, it makes sense to begin by understanding which values might go out
of range.</p>
<p>For those tensors in FP32 or BF16, range issues are unlikely to occur as these formats
can represent very large/small numbers (roughly 3e+38 to 1e-45).</p>
<p>Tensors in FP16 or FP8 are likely to require unit-scaling. FP16 and the FP8 E5
format can represent numbers between roughly 60,000 and 6e-05
(FP8 E4 has an even smaller range). Operations which use values in these formats may
require unit scaling.</p>
<p>We recommend that you try and put as many tensors as possible into low-precision formats as
this can speed up training considerably, and is where unit scaling is most useful.
A full discussion of which tensors should be in which format is beyond the scope of this
introduction.</p>
<p><strong>2. Analyse scaling</strong></p>
<p>The next step is to understand the scales present in the initial (non-unit-scaled)
model. This analysis can be tricky to implement, particularly in the backward pass, so
we provide a tool to make this analysis easier:
<a class="reference internal" href="generated/unit_scaling.utils.analyse_module.html#unit_scaling.utils.analyse_module" title="unit_scaling.utils.analyse_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">unit_scaling.utils.analyse_module()</span></code></a>.</p>
<p>Using <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#module-torch.fx" title="(in PyTorch v2.5)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.fx</span></code></a>, this provides a line-by-line breakdown of a given model,
alongside the scale of the output tensor for each operation, in both the forward and
backward pass.</p>
<p>For example, given the following implementation of an MLP layer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">unit_scaling.utils</span> <span class="kn">import</span> <span class="n">analyse_module</span>

<span class="k">class</span> <span class="nc">UnscaledMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>we can use <a class="reference internal" href="generated/unit_scaling.utils.analyse_module.html#unit_scaling.utils.analyse_module" title="unit_scaling.utils.analyse_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">analyse_module()</span></code></a> to derive the following
analysis:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>  <span class="c1"># fed into fwd pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bwd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># fed into bwd pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">annotated_code</span> <span class="o">=</span> <span class="n">analyse_module</span><span class="p">(</span><span class="n">UnscaledMLP</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">10</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">bwd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">annotated_code</span><span class="p">)</span>

<span class="go">def forward(self, x : torch.Tensor) -&gt; torch.Tensor:  (-&gt; 1.0, &lt;- 0.204)</span>
<span class="go">    linear_1_weight = self.linear_1.weight;  (-&gt; 0.018, &lt;- 2.83)</span>
<span class="go">    linear_1_bias = self.linear_1.bias;  (-&gt; 0.018, &lt;- 2.84)</span>
<span class="go">    linear = torch._C._nn.linear(x, linear_1_weight, linear_1_bias);  (-&gt; 0.578, &lt;- 0.177)</span>
<span class="go">    gelu = torch._C._nn.gelu(linear);  (-&gt; 0.322, &lt;- 0.289)</span>
<span class="go">    linear_2_weight = self.linear_2.weight;  (-&gt; 0.00902, &lt;- 5.48)</span>
<span class="go">    linear_2_bias = self.linear_2.bias;  (-&gt; 0.00894, &lt;- 16.1)</span>
<span class="go">    linear_1 = torch._C._nn.linear(gelu, linear_2_weight, linear_2_bias);  (-&gt; 0.198, &lt;- 1.0)</span>
<span class="go">    return linear_1</span>
</pre></div>
</div>
<p>Firstly, <a class="reference internal" href="generated/unit_scaling.utils.analyse_module.html#unit_scaling.utils.analyse_module" title="unit_scaling.utils.analyse_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">analyse_module()</span></code></a> has decomposed the module into a set of low-level
operations. Secondly, it has appended each line with a tuple
<code class="code docutils literal notranslate"><span class="pre">(-&gt;</span> <span class="pre">fwd_scale,</span> <span class="pre">&lt;-</span> <span class="pre">bwd_scale)</span></code> denoting the scale of the tensor on the left of
the <code class="code docutils literal notranslate"><span class="pre">=</span></code> sign in the forward and backward passes.</p>
<p>We can see from the above example that this module is not well-scaled. In both passes
we begin with a scale of 1 (as this is what we fed in). By the end of the forward pass
the scale is 0.198, and by the end of the backward pass the scale is 0.204. Along the
way we generate large scales for some of the weight gradients, with
<code class="code docutils literal notranslate"><span class="pre">linear_2_bias</span></code> receiving a gradient of scale 16.1.</p>
<p>These scales are not large or small enough to be a problem for our number formats, but in a
full model the unscaled operations could cause more significant numerical issues.
We show below how to address this using unit scaling.</p>
<p>(note: <a class="reference internal" href="generated/unit_scaling.utils.analyse_module.html#unit_scaling.utils.analyse_module" title="unit_scaling.utils.analyse_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">analyse_module()</span></code></a> can’t be used on a model wrapped in
<code class="code docutils literal notranslate"><span class="pre">torch.compile</span></code>)</p>
<p><strong>3. Swap in unit-scaled ops</strong></p>
<p>By swapping-in unit-scaled versions of the operations in the module, we can correct
these scaling factors. <code class="code docutils literal notranslate"><span class="pre">unit-scaling</span></code> provides drop-in replacements:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unit_scaling</span> <span class="k">as</span> <span class="nn">uu</span>
<span class="kn">import</span> <span class="nn">unit_scaling.functional</span> <span class="k">as</span> <span class="nn">U</span>

<span class="k">class</span> <span class="nc">ScaledMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">uu</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Changed `nn` to `uu`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">uu</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># Changed `nn` to `uu`</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Changed `F` to `U`</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">annotated_code</span> <span class="o">=</span> <span class="n">analyse_module</span><span class="p">(</span><span class="n">ScaledMLP</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">10</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">bwd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">annotated_code</span><span class="p">)</span>

<span class="go">def forward(self, x : torch.Tensor) -&gt; torch.Tensor:  (-&gt; 1.0, &lt;- 1.01)</span>
<span class="go">    linear_1_weight = self.linear_1.weight;  (-&gt; 1.0, &lt;- 0.716)</span>
<span class="go">    linear_1_bias = self.linear_1.bias;  (-&gt; 0.0, &lt;- 0.729)</span>
<span class="go">    linear = U.linear(x, linear_1_weight, linear_1_bias, gmean);  (-&gt; 0.707, &lt;- 0.716)</span>
<span class="go">    gelu = U.gelu(linear);  (-&gt; 0.64, &lt;- 0.706)</span>
<span class="go">    linear_2_weight = self.linear_2.weight;  (-&gt; 1.0, &lt;- 0.693)</span>
<span class="go">    linear_2_bias = self.linear_2.bias;  (-&gt; 0.0, &lt;- 1.03)</span>
<span class="go">    linear_1 = U.linear(gelu, linear_2_weight, linear_2_bias, gmean);  (-&gt; 0.979, &lt;- 0.999)</span>
<span class="go">    return linear_1</span>
</pre></div>
</div>
<p>Note that not all modules and functions are implemented in <code class="code docutils literal notranslate"><span class="pre">unit-scaling</span></code>.
Implementations of the basic operations required for a transformer are available, but
many other operations are not yet provided.</p>
<p>For the set of modules and functions currently implemented, see <a class="reference internal" href="api_reference.html#api-reference"><span class="std std-numref">Section 3. API reference</span></a>.</p>
<p><strong>4. Repeat steps 2 &amp; 3 until scales look good</strong></p>
<p>It’s important to check that swapping in unit-scaled ops has the desired effect on
the scales in a model. There may be cases in which this is not the case, and additional
measures are required.</p>
<p>Understanding when tensor scales are “good enough” is something of an art. Generally,
when the standard deviation begins to approach the max/min values defined by a format then
numerical issues arise. For overflow, this is typically seen clearly in the loss
exploding (even with gradient clipping). Conversely, underflow tends to cause the loss
to degrade more steadily.</p>
<p>It’s not necessary to keep scales at exactly 1, and unit-scaling is designed to only
approximately meet this target. In practice, scales of between 1/10 to 10 are of no
concern and are to be expected. Significantly smaller or larger scales may merit further
investigation (particularly larger).</p>
<p><strong>5. Optimise</strong></p>
<p>To attain the best performance, we recommend models are wrapped in
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> (requires PyTorch &gt;=2.0).
This is enabled via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Model</span><span class="p">())</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>As outlined in the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a>,
documentation, compilation is a general-purpose
optimisation for models. It’s particularly useful in the case of unit scaling, in order
to fuse scaling factors with operations
(see <a class="reference internal" href="#optimising-unit-scaled-models"><span class="std std-numref">Section 1.5. Optimising unit-scaled models</span></a> for more detail).</p>
</section>
<section id="key-considerations-for-unit-scaling">
<h2><span class="section-number">1.4. </span>Key considerations for unit scaling<a class="headerlink" href="#key-considerations-for-unit-scaling" title="Link to this heading"></a></h2>
<p><strong>Loss functions</strong></p>
<p>The most important operation in the model to unit-scale is the loss function.
The division term and log-softmax used in the standard cross-entropy loss tend to
shrink gradients substantially.
The implementation in <code class="code docutils literal notranslate"><span class="pre">unit_scaling</span></code> provides scaled versions of
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.cross_entropy()</span></code></a>
and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a>
which correct for this. We recommend that you start here when unit-scaling your models.</p>
<p><strong>Linear layers</strong></p>
<p>In non-unit-scaled models, linear layers have a mechanism for controlling the scale:
their initialisation. The standard Xavier/Glorot initialisation provides good scaling
for activations and their gradients by pushing a (small) scaling factor into the weights
themselves. However, it does not provide good scaling for weight gradients.</p>
<p>Unit scaling solves this problem by taking a different approach: keeping scaling factors
outside the weights, which then enables separate scaling factors for activation
gradients and weight gradients. Because of this, you should expect your weights
to begin with scale=1 when using <code class="code docutils literal notranslate"><span class="pre">unit_scaling</span></code>. Alternative weight
initialisations should not be used in conjunction with unit scaling.</p>
<p><strong>Residual layers</strong></p>
<p>Particular care must be taken when using residual connections in unit-scaled models.
We provide two methods for residual scaling, which must be used together.</p>
<p>Consider a PyTorch residual layer of the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResidualLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">skip</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">skip</span>
</pre></div>
</div>
<p>The unit-scaled equivalent should be implemented as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResidualLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span><span class="p">,</span> <span class="n">skip</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">residual_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">U</span><span class="o">.</span><span class="n">residual_add</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span> <span class="n">skip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>This step is necessary because unit-scaled models give equal scale to the skip and
residual connections. In contrast, non-unit-scaled models tend change the scale of
activations as they go through the residual connection, meaning that when the residual
connection is added to the skip connection the ratio of the two scales is not 50:50.</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">tau</span></code> hyperparameter is a scale-factor applied to the residual branch to
correct for this. In practice you may be able to leave it at the default value of 0.5
without having to tune this as an additional hyperparameter.</p>
<p>However in the case of self-attention layers, we find that tau must be dropped to
approximately 0.01. The default of 0.5 (which weights the branches 50:50) causes
significant degradation. This reflects the fact that in standard transformers the
self-attention layer down-scales the residual branch. Note that for MLP layers the
default tau=0.5 is sufficient.</p>
<p>We also employ a trick to ensure that this scaling factor is delayed in the backward
pass to keep values unit-scaled along the residual branch in both passes
(see <a class="reference internal" href="generated/unit_scaling.functional.residual_split.html#unit_scaling.functional.residual_split" title="unit_scaling.functional.residual_split"><code class="xref py py-func docutils literal notranslate"><span class="pre">residual_split()</span></code></a> for further details).
A more comprehensive discussion of this feature can be found in the
<a class="reference external" href="https://arxiv.org/abs/2303.11257">unit scaling paper</a>.</p>
<p><strong>Constraints</strong></p>
<p>Many unit-scaled operations introduce a <code class="code docutils literal notranslate"><span class="pre">constraint:</span> <span class="pre">Callable</span></code> argument.
<em>In most cases, you can simply leave this argument to take the default value and ignore it.</em></p>
<p>The purpose of this constraint is that in some scenarios,
particular scaling factors in the
forward and backward passes must all be identical in order to produce
valid gradients. This constraint argument specifies how to arrive at the shared scale.</p>
<p>For example, the implementation of <a class="reference internal" href="generated/unit_scaling.functional.linear.html#unit_scaling.functional.linear" title="unit_scaling.functional.linear"><code class="xref py py-func docutils literal notranslate"><span class="pre">unit_scaling.functional.linear()</span></code></a> contains the
following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output_scale</span> <span class="o">=</span> <span class="n">fan_in</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="n">grad_input_scale</span> <span class="o">=</span> <span class="n">fan_out</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="n">grad_weight_scale</span> <span class="o">=</span> <span class="n">grad_bias_scale</span> <span class="o">=</span> <span class="n">batch_size</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="k">if</span> <span class="n">constraint</span><span class="p">:</span>
    <span class="n">output_scale</span> <span class="o">=</span> <span class="n">grad_input_scale</span> <span class="o">=</span> <span class="n">constraint</span><span class="p">(</span><span class="n">output_scale</span><span class="p">,</span> <span class="n">grad_input_scale</span><span class="p">)</span>
</pre></div>
</div>
<p>First the “ideal” output and input-gradient scales are computed, and are then combined
using the provided constraint (if one is supplied). Constraining these values to be
the same for a linear layer is necessary to ensure valid gradients. This can cause
deviations from exact unit-scale, but these tend not to be significant.</p>
<p>The default value of <code class="code docutils literal notranslate"><span class="pre">constraint</span></code> is typically
<a class="reference internal" href="generated/unit_scaling.constraints.gmean.html#unit_scaling.constraints.gmean" title="unit_scaling.constraints.gmean"><code class="xref py py-func docutils literal notranslate"><span class="pre">unit_scaling.constraints.gmean()</span></code></a>
(the geometric mean), representing a compromise between the forward and backward passes.
Note that we don’t need to constrain the weight scale as this is allowed to
differ from the output/input-grad scales.</p>
<p>The <a class="reference external" href="https://arxiv.org/abs/2303.11257">unit scaling paper</a> provides a comprehensive overview of where and why
constraints are required.</p>
</section>
<section id="optimising-unit-scaled-models">
<h2><span class="section-number">1.5. </span>Optimising unit-scaled models<a class="headerlink" href="#optimising-unit-scaled-models" title="Link to this heading"></a></h2>
<p>Unit scaling adds extra scalar multiplications to each operation.
By default, PyTorch’s eager evaluation causes each of these multiplications to make an
additional trip to-and-from memory.</p>
<p>Fortunately, his overhead can be eliminated via <em>kernel fusion</em>
(see this <a class="reference external" href="https://stackoverflow.com/a/53311373">Stack Overflow answer</a>
for more details). In PyTorch there are two ways of fusing operations.</p>
<p>The “old” method uses <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code></a> to convert PyTorch into a TorchScript
program, which is then just-in-time compiled.
However, many models can’t be converted to TorchScript directly.</p>
<p>To rectify this, PyTorch 2.0 introduced a new method: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.
This approach is much more flexible and in theory can work on
arbitrary PyTorch programs. It can be applied to functions or modules as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">unit_scaled_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="o">...</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">class</span> <span class="nc">UnitScaledModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Please refer to the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile
tutorial</a>
for further details.</p>
<p>For unit scaling, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> fuses scaling factors where possible in the
forward and backward passes. This removes the overhead incurred when naively
adding scaling factors without fusion
(see the
<a class="reference external" href="https://github.com/graphcore-research/unit-scaling/tree/main/analysis/benchmarking_compiled_unit_scaled_ops.ipynb">benchmarking compiled unit-scaled ops</a>
notebook for a thorough analysis).</p>
<p>:code`unit-scaling` does not automatically apply
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>, so users will have to do this manually.
We strongly recommend users consider doing so
in order to get the most substantial speedups,
ideally in large blocks or compiling the entire model.</p>
<p>Note that there’s a bug in the latest PyTorch version (&lt;= 2.0.1) meaning the backward pass
fails to fuse scaling factors. This has recently been addressed, but
users will need to upgrade to the
<a class="reference external" href="https://pytorch.org/get-started/locally/">Preview (Nightly) build</a> (until
PyTorch 2.0.2 is released) to get the fix.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Unit Scaling" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="limitations.html" class="btn btn-neutral float-right" title="2. Limitations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright (c) 2023 Graphcore Ltd. All rights reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>