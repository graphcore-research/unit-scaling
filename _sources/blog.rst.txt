Unit Scaling blog
=================

:doc:`Almost scaled dot-product self attention <posts/almost_scaled_dot_product_attention>`
------------

    Transformers seem to be all you need, but we don't fully understand why they work so well. While working on `unit scaling <https://arxiv.org/abs/2303.11257>`_, we noticed something surprising about attention, the heart of the transformer architecture, and how the outputs are scaled...

    .. image:: posts/img/attention_scaling.png
        :width: 400
        :alt: diagram showing an alternative attention scaling
        :target: posts/almost_scaled_dot_product_attention.html

    :guilabel:`Douglas Orr` :guilabel:`October, 2023`