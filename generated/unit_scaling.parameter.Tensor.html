

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3.1.24.6. unit_scaling.parameter.Tensor &mdash; unit-scaling  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
    <link rel="shortcut icon" href="../_static/scales.png"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.2. unit_scaling.analysis" href="unit_scaling.analysis.html" />
    <link rel="prev" title="3.1.24.5. unit_scaling.parameter.Protocol" href="unit_scaling.parameter.Protocol.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            unit-scaling
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../user_guide.html">1. User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">2. Limitations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api_reference.html">3. API reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="unit_scaling.html">3.1. unit_scaling</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Parameter.html">3.1.1. unit_scaling.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.transformer_residual_scaling_rule.html">3.1.2. unit_scaling.transformer_residual_scaling_rule</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.visualiser.html">3.1.3. unit_scaling.visualiser</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Conv1d.html">3.1.4. unit_scaling.Conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.CrossEntropyLoss.html">3.1.5. unit_scaling.CrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.DepthModuleList.html">3.1.6. unit_scaling.DepthModuleList</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.DepthSequential.html">3.1.7. unit_scaling.DepthSequential</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Dropout.html">3.1.8. unit_scaling.Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Embedding.html">3.1.9. unit_scaling.Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.GELU.html">3.1.10. unit_scaling.GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.LayerNorm.html">3.1.11. unit_scaling.LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Linear.html">3.1.12. unit_scaling.Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.LinearReadout.html">3.1.13. unit_scaling.LinearReadout</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.MHSA.html">3.1.14. unit_scaling.MHSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.MLP.html">3.1.15. unit_scaling.MLP</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.RMSNorm.html">3.1.16. unit_scaling.RMSNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.SiLU.html">3.1.17. unit_scaling.SiLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.Softmax.html">3.1.18. unit_scaling.Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.TransformerDecoder.html">3.1.19. unit_scaling.TransformerDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.TransformerLayer.html">3.1.20. unit_scaling.TransformerLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.core.html">3.1.21. unit_scaling.core</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.functional.html">3.1.22. unit_scaling.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="unit_scaling.optim.html">3.1.23. unit_scaling.optim</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="unit_scaling.parameter.html">3.1.24. unit_scaling.parameter</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="unit_scaling.parameter.Parameter.html">3.1.24.1. unit_scaling.parameter.Parameter</a></li>
<li class="toctree-l4"><a class="reference internal" href="unit_scaling.parameter.has_parameter_data.html">3.1.24.2. unit_scaling.parameter.has_parameter_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="unit_scaling.parameter.OrderedDict.html">3.1.24.3. unit_scaling.parameter.OrderedDict</a></li>
<li class="toctree-l4"><a class="reference internal" href="unit_scaling.parameter.ParameterData.html">3.1.24.4. unit_scaling.parameter.ParameterData</a></li>
<li class="toctree-l4"><a class="reference internal" href="unit_scaling.parameter.Protocol.html">3.1.24.5. unit_scaling.parameter.Protocol</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">3.1.24.6. unit_scaling.parameter.Tensor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.analysis.html">3.2. unit_scaling.analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.constraints.html">3.3. unit_scaling.constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.formats.html">3.4. unit_scaling.formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.functional.html">3.1.22. unit_scaling.functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.optim.html">3.1.23. unit_scaling.optim</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.scale.html">3.5. unit_scaling.scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.transforms.html">3.6. unit_scaling.transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.transforms.utils.html">3.7. unit_scaling.transforms.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.utils.html">3.8. unit_scaling.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="unit_scaling.core.functional.html">3.1.21.1. unit_scaling.core.functional</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">unit-scaling</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api_reference.html"><span class="section-number">3. </span>API reference</a></li>
          <li class="breadcrumb-item"><a href="unit_scaling.html"><span class="section-number">3.1. </span>unit_scaling</a></li>
          <li class="breadcrumb-item"><a href="unit_scaling.parameter.html"><span class="section-number">3.1.24. </span>unit_scaling.parameter</a></li>
      <li class="breadcrumb-item active"><span class="section-number">3.1.24.6. </span>unit_scaling.parameter.Tensor</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/unit_scaling.parameter.Tensor.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="unit-scaling-parameter-tensor">
<h1><span class="section-number">3.1.24.6. </span>unit_scaling.parameter.Tensor<a class="headerlink" href="#unit-scaling-parameter-tensor" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">unit_scaling.parameter.</span></span><span class="sig-name descname"><span class="pre">Tensor</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor" title="Link to this definition"></a></dt>
<dd><dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.H">
<span class="sig-name descname"><span class="pre">H</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.H" title="Link to this definition"></a></dt>
<dd><p>Returns a view of a matrix (2-D tensor) conjugated and transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.H</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1).conj()</span></code> for complex matrices and
<code class="docutils literal notranslate"><span class="pre">x.transpose(0,</span> <span class="pre">1)</span></code> for real matrices.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.mH" title="unit_scaling.parameter.Tensor.mH"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mH</span></code></a>: An attribute that also works on batches of matrices.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.T">
<span class="sig-name descname"><span class="pre">T</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.T" title="Link to this definition"></a></dt>
<dd><p>Returns a view of this tensor with its dimensions reversed.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of dimensions in <code class="docutils literal notranslate"><span class="pre">x</span></code>,
<code class="docutils literal notranslate"><span class="pre">x.T</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.permute(n-1,</span> <span class="pre">n-2,</span> <span class="pre">...,</span> <span class="pre">0)</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The use of <a class="reference internal" href="#unit_scaling.parameter.Tensor.T" title="unit_scaling.parameter.Tensor.T"><code class="xref py py-func docutils literal notranslate"><span class="pre">Tensor.T()</span></code></a> on tensors of dimension other than 2 to reverse their shape
is deprecated and it will throw an error in a future release. Consider <a class="reference internal" href="#unit_scaling.parameter.Tensor.mT" title="unit_scaling.parameter.Tensor.mT"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mT</span></code></a>
to transpose batches of matrices or <cite>x.permute(*torch.arange(x.ndim - 1, -1, -1))</cite> to reverse
the dimensions of a tensor.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.abs">
<span class="sig-name descname"><span class="pre">abs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.abs" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.abs_">
<span class="sig-name descname"><span class="pre">abs_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.abs_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.abs" title="unit_scaling.parameter.Tensor.abs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.absolute">
<span class="sig-name descname"><span class="pre">absolute</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.absolute" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.abs" title="unit_scaling.parameter.Tensor.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.absolute_">
<span class="sig-name descname"><span class="pre">absolute_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.absolute_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.absolute" title="unit_scaling.parameter.Tensor.absolute"><code class="xref py py-meth docutils literal notranslate"><span class="pre">absolute()</span></code></a>
Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.abs_" title="unit_scaling.parameter.Tensor.abs_"><code class="xref py py-func docutils literal notranslate"><span class="pre">abs_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.acos">
<span class="sig-name descname"><span class="pre">acos</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.acos" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.acos_">
<span class="sig-name descname"><span class="pre">acos_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.acos_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.acos" title="unit_scaling.parameter.Tensor.acos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.acosh">
<span class="sig-name descname"><span class="pre">acosh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.acosh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.acosh_">
<span class="sig-name descname"><span class="pre">acosh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.acosh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.acosh" title="unit_scaling.parameter.Tensor.acosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.add" title="Link to this definition"></a></dt>
<dd><p>Add a scalar or tensor to <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If both <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcastable</span></a> with the shape of the underlying
tensor</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.add.html#torch.add" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.add_">
<span class="sig-name descname"><span class="pre">add_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.add_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.add" title="unit_scaling.parameter.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addbmm">
<span class="sig-name descname"><span class="pre">addbmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addbmm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addbmm_">
<span class="sig-name descname"><span class="pre">addbmm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addbmm_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addbmm" title="unit_scaling.parameter.Tensor.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addcdiv">
<span class="sig-name descname"><span class="pre">addcdiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addcdiv" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addcdiv_">
<span class="sig-name descname"><span class="pre">addcdiv_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addcdiv_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addcdiv" title="unit_scaling.parameter.Tensor.addcdiv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addcmul">
<span class="sig-name descname"><span class="pre">addcmul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addcmul" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addcmul_">
<span class="sig-name descname"><span class="pre">addcmul_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addcmul_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addcmul" title="unit_scaling.parameter.Tensor.addcmul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addmm">
<span class="sig-name descname"><span class="pre">addmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addmm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addmm_">
<span class="sig-name descname"><span class="pre">addmm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addmm_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addmm" title="unit_scaling.parameter.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addmv">
<span class="sig-name descname"><span class="pre">addmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addmv" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addmv_">
<span class="sig-name descname"><span class="pre">addmv_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addmv_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addmv" title="unit_scaling.parameter.Tensor.addmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addr">
<span class="sig-name descname"><span class="pre">addr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addr" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.addr_">
<span class="sig-name descname"><span class="pre">addr_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.addr_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.addr" title="unit_scaling.parameter.Tensor.addr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.adjoint">
<span class="sig-name descname"><span class="pre">adjoint</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.adjoint" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.adjoint" title="unit_scaling.parameter.Tensor.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.align_as">
<span class="sig-name descname"><span class="pre">align_as</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.align_as" title="Link to this definition"></a></dt>
<dd><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the dimension order
in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> tensor, adding size-one dims for any new names.</p>
<p>This operation is useful for explicit broadcasting by names (see examples).</p>
<p>All of the dims of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be named in order to use this method.
The resulting tensor is a view on the original tensor.</p>
<p>All dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be present in <code class="docutils literal notranslate"><span class="pre">other.names</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> may contain named dimensions that are not in <code class="docutils literal notranslate"><span class="pre">self.names</span></code>;
the output tensor has a size-one dimension for each of those new names.</p>
<p>To align a tensor to a specific order, use <a class="reference internal" href="#unit_scaling.parameter.Tensor.align_to" title="unit_scaling.parameter.Tensor.align_to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">align_to()</span></code></a>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example 1: Applying a mask</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">align_as</span><span class="p">(</span><span class="n">imgs</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>


<span class="c1"># Example 2: Applying a per-channel-scale</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">scale_channels</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="k">return</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">scale</span><span class="o">.</span><span class="n">align_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">num_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_channels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">more_imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">videos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">))</span>

<span class="c1"># scale_channels is agnostic to the dimension order of the input</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">more_imgs</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scale_channels</span><span class="p">(</span><span class="n">videos</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.align_to">
<span class="sig-name descname"><span class="pre">align_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.align_to"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.align_to" title="Link to this definition"></a></dt>
<dd><p>Permutes the dimensions of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to match the order
specified in <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>, adding size-one dims for any new names.</p>
<p>All of the dims of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be named in order to use this method.
The resulting tensor is a view on the original tensor.</p>
<p>All dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must be present in <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.
<a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain additional names that are not in <code class="docutils literal notranslate"><span class="pre">self.names</span></code>;
the output tensor has a size-one dimension for each of those new names.</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded to be equal to all dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
that are not mentioned in <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>, in the order that they appear
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>names</strong> (<em>iterable</em><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The desired dimension ordering of the
output tensor. May contain up to one Ellipsis that is expanded
to all unmentioned dim names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">)</span>

<span class="go"># Move the F and E dims to the front while keeping the rest in order</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.all">
<span class="sig-name descname"><span class="pre">all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.all" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.all.html#torch.all" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.all()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.allclose">
<span class="sig-name descname"><span class="pre">allclose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.allclose" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.amax">
<span class="sig-name descname"><span class="pre">amax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.amax" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amax()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.amin">
<span class="sig-name descname"><span class="pre">amin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.amin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.amin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.aminmax">
<span class="sig-name descname"><span class="pre">aminmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim=False)</span> <span class="pre">-&gt;</span> <span class="pre">(Tensor</span> <span class="pre">min</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Tensor</span> <span class="pre">max</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.aminmax" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.aminmax()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.angle">
<span class="sig-name descname"><span class="pre">angle</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.angle" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.angle()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.any">
<span class="sig-name descname"><span class="pre">any</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.any" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.any.html#torch.any" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.any()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.apply_">
<span class="sig-name descname"><span class="pre">apply_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">callable</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.apply_" title="Link to this definition"></a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arccos">
<span class="sig-name descname"><span class="pre">arccos</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arccos" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arccos_">
<span class="sig-name descname"><span class="pre">arccos_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arccos_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arccos" title="unit_scaling.parameter.Tensor.arccos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arccos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arccosh">
<span class="sig-name descname"><span class="pre">arccosh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arccosh" title="Link to this definition"></a></dt>
<dd><p>acosh() -&gt; Tensor</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arccosh.html#torch.arccosh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arccosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arccosh_">
<span class="sig-name descname"><span class="pre">arccosh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arccosh_" title="Link to this definition"></a></dt>
<dd><p>acosh_() -&gt; Tensor</p>
<p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arccosh" title="unit_scaling.parameter.Tensor.arccosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arccosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arcsin">
<span class="sig-name descname"><span class="pre">arcsin</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arcsin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arcsin_">
<span class="sig-name descname"><span class="pre">arcsin_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arcsin_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arcsin" title="unit_scaling.parameter.Tensor.arcsin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arcsinh">
<span class="sig-name descname"><span class="pre">arcsinh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arcsinh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arcsinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arcsinh_">
<span class="sig-name descname"><span class="pre">arcsinh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arcsinh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arcsinh" title="unit_scaling.parameter.Tensor.arcsinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arcsinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctan">
<span class="sig-name descname"><span class="pre">arctan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctan" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctan2">
<span class="sig-name descname"><span class="pre">arctan2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctan2" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctan2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctan2_">
<span class="sig-name descname"><span class="pre">arctan2_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctan2_" title="Link to this definition"></a></dt>
<dd><p>atan2_(other) -&gt; Tensor</p>
<p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arctan2" title="unit_scaling.parameter.Tensor.arctan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctan_">
<span class="sig-name descname"><span class="pre">arctan_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctan_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arctan" title="unit_scaling.parameter.Tensor.arctan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arctan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctanh">
<span class="sig-name descname"><span class="pre">arctanh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctanh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arctanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.arctanh_">
<span class="sig-name descname"><span class="pre">arctanh_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.arctanh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.arctanh" title="unit_scaling.parameter.Tensor.arctanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">arctanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.argmax">
<span class="sig-name descname"><span class="pre">argmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.argmax" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.argmin">
<span class="sig-name descname"><span class="pre">argmin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.argmin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.argsort">
<span class="sig-name descname"><span class="pre">argsort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.argsort" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argsort()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.argwhere">
<span class="sig-name descname"><span class="pre">argwhere</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.argwhere" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argwhere()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.as_strided">
<span class="sig-name descname"><span class="pre">as_strided</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.as_strided" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.as_strided_">
<span class="sig-name descname"><span class="pre">as_strided_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.as_strided_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.as_strided" title="unit_scaling.parameter.Tensor.as_strided"><code class="xref py py-meth docutils literal notranslate"><span class="pre">as_strided()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.as_strided_scatter">
<span class="sig-name descname"><span class="pre">as_strided_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.as_strided_scatter" title="Link to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided_scatter()</span></code></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.as_subclass">
<span class="sig-name descname"><span class="pre">as_subclass</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.as_subclass" title="Link to this definition"></a></dt>
<dd><p>Makes a <code class="docutils literal notranslate"><span class="pre">cls</span></code> instance with the same data pointer as <code class="docutils literal notranslate"><span class="pre">self</span></code>. Changes
in the output mirror changes in <code class="docutils literal notranslate"><span class="pre">self</span></code>, and the output stays attached
to the autograd graph. <code class="docutils literal notranslate"><span class="pre">cls</span></code> must be a subclass of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.asin">
<span class="sig-name descname"><span class="pre">asin</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.asin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.asin_">
<span class="sig-name descname"><span class="pre">asin_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.asin_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.asin" title="unit_scaling.parameter.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.asinh">
<span class="sig-name descname"><span class="pre">asinh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.asinh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.asinh_">
<span class="sig-name descname"><span class="pre">asinh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.asinh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.asinh" title="unit_scaling.parameter.Tensor.asinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atan">
<span class="sig-name descname"><span class="pre">atan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atan" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atan2">
<span class="sig-name descname"><span class="pre">atan2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atan2" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atan2_">
<span class="sig-name descname"><span class="pre">atan2_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atan2_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.atan2" title="unit_scaling.parameter.Tensor.atan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atan_">
<span class="sig-name descname"><span class="pre">atan_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atan_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.atan" title="unit_scaling.parameter.Tensor.atan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atanh">
<span class="sig-name descname"><span class="pre">atanh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atanh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.atanh_">
<span class="sig-name descname"><span class="pre">atanh_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.atanh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.atanh" title="unit_scaling.parameter.Tensor.atanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradient</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.backward" title="Link to this definition"></a></dt>
<dd><p>Computes the gradient of current tensor wrt graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the tensor is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionally requires specifying a <code class="docutils literal notranslate"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and shape, that represents
the gradient of the differentiated function w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to zero
<code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes or set them to <code class="docutils literal notranslate"><span class="pre">None</span></code> before calling it.
See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#default-grad-layouts" title="(in PyTorch v2.5)"><span class="xref std std-ref">Default gradient layouts</span></a>
for details on the memory layout of accumulated gradients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you run any forward ops, create <code class="docutils literal notranslate"><span class="pre">gradient</span></code>, and/or call <code class="docutils literal notranslate"><span class="pre">backward</span></code>
in a user-specified CUDA stream context, see
<a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#bwd-cuda-stream-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">Stream semantics of backward passes</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are provided and a given input is not a leaf,
the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).
It is an implementation detail on which the user should not rely.
See <a class="reference external" href="https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780">https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780</a> for more details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gradient</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – The gradient of the function
being differentiated w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.
This argument can be omitted if <code class="docutils literal notranslate"><span class="pre">self</span></code> is a scalar.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute
the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p></li>
<li><p><strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will
be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>inputs</strong> (<em>sequence</em><em> of </em><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – Inputs w.r.t. which the gradient will be
accumulated into <code class="docutils literal notranslate"><span class="pre">.grad</span></code>. All other tensors will be ignored. If not
provided, the gradient is accumulated into all the leaf Tensors that were
used to compute the <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.baddbmm">
<span class="sig-name descname"><span class="pre">baddbmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.baddbmm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.baddbmm_">
<span class="sig-name descname"><span class="pre">baddbmm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.baddbmm_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.baddbmm" title="unit_scaling.parameter.Tensor.baddbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bernoulli">
<span class="sig-name descname"><span class="pre">bernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bernoulli" title="Link to this definition"></a></dt>
<dd><p>Returns a result tensor where each <span class="math notranslate nohighlight">\(\texttt{result[i]}\)</span> is independently
sampled from <span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{self[i]})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, and the result will have the same <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bernoulli_">
<span class="sig-name descname"><span class="pre">bernoulli_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bernoulli_" title="Link to this definition"></a></dt>
<dd><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from
<span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{p})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral
<code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> should either be a scalar or tensor containing probabilities to be
used for drawing the binary random number.</p>
<p>If it is a tensor, the <span class="math notranslate nohighlight">\(\text{i}^{th}\)</span> element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor
will be set to a value sampled from
<span class="math notranslate nohighlight">\(\text{Bernoulli}(\texttt{p\_tensor[i]})\)</span>. In this case <cite>p</cite> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.bernoulli" title="unit_scaling.parameter.Tensor.bernoulli"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bernoulli()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bfloat16" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bfloat16()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bfloat16)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bincount">
<span class="sig-name descname"><span class="pre">bincount</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minlength</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bincount" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bincount()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_and">
<span class="sig-name descname"><span class="pre">bitwise_and</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_and" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_and()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_and_">
<span class="sig-name descname"><span class="pre">bitwise_and_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_and_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_and" title="unit_scaling.parameter.Tensor.bitwise_and"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_and()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_left_shift">
<span class="sig-name descname"><span class="pre">bitwise_left_shift</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_left_shift" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_left_shift()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_left_shift_">
<span class="sig-name descname"><span class="pre">bitwise_left_shift_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_left_shift_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_left_shift" title="unit_scaling.parameter.Tensor.bitwise_left_shift"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_left_shift()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_not">
<span class="sig-name descname"><span class="pre">bitwise_not</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_not" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_not()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_not_">
<span class="sig-name descname"><span class="pre">bitwise_not_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_not_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_not" title="unit_scaling.parameter.Tensor.bitwise_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_not()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_or">
<span class="sig-name descname"><span class="pre">bitwise_or</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_or" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_or()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_or_">
<span class="sig-name descname"><span class="pre">bitwise_or_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_or_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_or" title="unit_scaling.parameter.Tensor.bitwise_or"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_or()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_right_shift">
<span class="sig-name descname"><span class="pre">bitwise_right_shift</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_right_shift" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_right_shift()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_right_shift_">
<span class="sig-name descname"><span class="pre">bitwise_right_shift_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_right_shift_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_right_shift" title="unit_scaling.parameter.Tensor.bitwise_right_shift"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_right_shift()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_xor">
<span class="sig-name descname"><span class="pre">bitwise_xor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_xor" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bitwise_xor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bitwise_xor_">
<span class="sig-name descname"><span class="pre">bitwise_xor_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bitwise_xor_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.bitwise_xor" title="unit_scaling.parameter.Tensor.bitwise_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bmm">
<span class="sig-name descname"><span class="pre">bmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bmm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.bool">
<span class="sig-name descname"><span class="pre">bool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.bool" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.bool()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.bool)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.broadcast_to">
<span class="sig-name descname"><span class="pre">broadcast_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.broadcast_to" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.broadcast_to()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.byte">
<span class="sig-name descname"><span class="pre">byte</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.byte" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cauchy_">
<span class="sig-name descname"><span class="pre">cauchy_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">median</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cauchy_" title="Link to this definition"></a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sigma (<span class="math notranslate nohighlight">\(\sigma\)</span>) is used to denote the scale parameter in Cauchy distribution.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cdouble">
<span class="sig-name descname"><span class="pre">cdouble</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cdouble" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.cdouble()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex128)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ceil">
<span class="sig-name descname"><span class="pre">ceil</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ceil" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ceil_">
<span class="sig-name descname"><span class="pre">ceil_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ceil_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.ceil" title="unit_scaling.parameter.Tensor.ceil"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cfloat">
<span class="sig-name descname"><span class="pre">cfloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cfloat" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.cfloat()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex64)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.chalf">
<span class="sig-name descname"><span class="pre">chalf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.chalf" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.chalf()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.complex32)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.char">
<span class="sig-name descname"><span class="pre">char</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.char" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cholesky">
<span class="sig-name descname"><span class="pre">cholesky</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cholesky" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cholesky_inverse">
<span class="sig-name descname"><span class="pre">cholesky_inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cholesky_inverse" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_inverse()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cholesky_solve">
<span class="sig-name descname"><span class="pre">cholesky_solve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cholesky_solve" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.chunk">
<span class="sig-name descname"><span class="pre">chunk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.chunk" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.clamp">
<span class="sig-name descname"><span class="pre">clamp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.clamp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.clamp_">
<span class="sig-name descname"><span class="pre">clamp_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.clamp_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.clamp" title="unit_scaling.parameter.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.clip">
<span class="sig-name descname"><span class="pre">clip</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.clip" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.clamp" title="unit_scaling.parameter.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.clip_">
<span class="sig-name descname"><span class="pre">clip_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.clip_" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.clamp_" title="unit_scaling.parameter.Tensor.clamp_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.clone">
<span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.clone" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.coalesce">
<span class="sig-name descname"><span class="pre">coalesce</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.coalesce" title="Link to this definition"></a></dt>
<dd><p>Returns a coalesced copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is an
<a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-uncoalesced-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">uncoalesced tensor</span></a>.</p>
<p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a coalesced tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.col_indices">
<span class="sig-name descname"><span class="pre">col_indices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">IntTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.col_indices" title="Link to this definition"></a></dt>
<dd><p>Returns the tensor containing the column indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.
The <code class="docutils literal notranslate"><span class="pre">col_indices</span></code> tensor is strictly of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.nnz())
and of type <code class="docutils literal notranslate"><span class="pre">int32</span></code> or <code class="docutils literal notranslate"><span class="pre">int64</span></code>.  When using MKL routines such as sparse
matrix multiplication, it is necessary to use <code class="docutils literal notranslate"><span class="pre">int32</span></code> indexing in order
to avoid downcasting and potentially losing information.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">col_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 2, 3, 4], dtype=torch.int32)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.conj">
<span class="sig-name descname"><span class="pre">conj</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.conj" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.conj_physical">
<span class="sig-name descname"><span class="pre">conj_physical</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.conj_physical" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.conj_physical()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.conj_physical_">
<span class="sig-name descname"><span class="pre">conj_physical_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.conj_physical_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.conj_physical" title="unit_scaling.parameter.Tensor.conj_physical"><code class="xref py py-meth docutils literal notranslate"><span class="pre">conj_physical()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.contiguous">
<span class="sig-name descname"><span class="pre">contiguous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.contiguous" title="Link to this definition"></a></dt>
<dd><p>Returns a contiguous in memory tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is already in the specified memory format, this function returns the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.copy_">
<span class="sig-name descname"><span class="pre">copy_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.copy_" title="Link to this definition"></a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor must be <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcastable</span></a>
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. It may be of a different data type or reside on a
different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the source tensor to copy from</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.copysign">
<span class="sig-name descname"><span class="pre">copysign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.copysign" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.copysign()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.copysign_">
<span class="sig-name descname"><span class="pre">copysign_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.copysign_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.copysign" title="unit_scaling.parameter.Tensor.copysign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copysign()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.corrcoef">
<span class="sig-name descname"><span class="pre">corrcoef</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.corrcoef" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.corrcoef()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cos">
<span class="sig-name descname"><span class="pre">cos</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cos" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cos_">
<span class="sig-name descname"><span class="pre">cos_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cos_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.cos" title="unit_scaling.parameter.Tensor.cos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cosh">
<span class="sig-name descname"><span class="pre">cosh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cosh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cosh_">
<span class="sig-name descname"><span class="pre">cosh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cosh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.cosh" title="unit_scaling.parameter.Tensor.cosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.count_nonzero">
<span class="sig-name descname"><span class="pre">count_nonzero</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.count_nonzero" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.count_nonzero()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cov">
<span class="sig-name descname"><span class="pre">cov</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fweights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aweights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cov" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cov()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cpu" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of this object in CPU memory.</p>
<p>If this object is already in CPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cross">
<span class="sig-name descname"><span class="pre">cross</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cross" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.crow_indices">
<span class="sig-name descname"><span class="pre">crow_indices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">IntTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.crow_indices" title="Link to this definition"></a></dt>
<dd><p>Returns the tensor containing the compressed row indices of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor when <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a sparse CSR tensor of layout <code class="docutils literal notranslate"><span class="pre">sparse_csr</span></code>.
The <code class="docutils literal notranslate"><span class="pre">crow_indices</span></code> tensor is strictly of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.size(0) + 1)
and of type <code class="docutils literal notranslate"><span class="pre">int32</span></code> or <code class="docutils literal notranslate"><span class="pre">int64</span></code>. When using MKL routines such as sparse
matrix multiplication, it is necessary to use <code class="docutils literal notranslate"><span class="pre">int32</span></code> indexing in order
to avoid downcasting and potentially losing information.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cuda" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination GPU device.
Defaults to the current CUDA device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cummax">
<span class="sig-name descname"><span class="pre">cummax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cummax" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummax()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cummin">
<span class="sig-name descname"><span class="pre">cummin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cummin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cummin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cumprod">
<span class="sig-name descname"><span class="pre">cumprod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cumprod" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cumprod_">
<span class="sig-name descname"><span class="pre">cumprod_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cumprod_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.cumprod" title="unit_scaling.parameter.Tensor.cumprod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cumprod()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cumsum">
<span class="sig-name descname"><span class="pre">cumsum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cumsum" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.cumsum_">
<span class="sig-name descname"><span class="pre">cumsum_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.cumsum_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.cumsum" title="unit_scaling.parameter.Tensor.cumsum"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cumsum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.data_ptr">
<span class="sig-name descname"><span class="pre">data_ptr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.data_ptr" title="Link to this definition"></a></dt>
<dd><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.deg2rad">
<span class="sig-name descname"><span class="pre">deg2rad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.deg2rad" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.deg2rad()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.deg2rad_">
<span class="sig-name descname"><span class="pre">deg2rad_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.deg2rad_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.deg2rad" title="unit_scaling.parameter.Tensor.deg2rad"><code class="xref py py-meth docutils literal notranslate"><span class="pre">deg2rad()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dense_dim">
<span class="sig-name descname"><span class="pre">dense_dim</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dense_dim" title="Link to this definition"></a></dt>
<dd><p>Return the number of dense dimensions in a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">len(self.shape)</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.sparse_dim" title="unit_scaling.parameter.Tensor.sparse_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.sparse_dim()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-hybrid-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">hybrid tensors</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dequantize">
<span class="sig-name descname"><span class="pre">dequantize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dequantize" title="Link to this definition"></a></dt>
<dd><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.det">
<span class="sig-name descname"><span class="pre">det</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.det" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.det.html#torch.det" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.detach">
<span class="sig-name descname"><span class="pre">detach</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.detach" title="Link to this definition"></a></dt>
<dd><p>Returns a new Tensor, detached from the current graph.</p>
<p>The result will never require gradient.</p>
<p>This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returned Tensor shares the same storage with the original one.
In-place modifications on either of them will be seen, and may trigger
errors in correctness checks.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.detach_">
<span class="sig-name descname"><span class="pre">detach_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.detach_" title="Link to this definition"></a></dt>
<dd><p>Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.</p>
<p>This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.device">
<span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.device" title="Link to this definition"></a></dt>
<dd><p>Is the <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> where this Tensor is.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diag">
<span class="sig-name descname"><span class="pre">diag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diag" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diag_embed">
<span class="sig-name descname"><span class="pre">diag_embed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diag_embed" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diagflat">
<span class="sig-name descname"><span class="pre">diagflat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diagflat" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diagonal">
<span class="sig-name descname"><span class="pre">diagonal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diagonal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diagonal_scatter">
<span class="sig-name descname"><span class="pre">diagonal_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diagonal_scatter" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal_scatter()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.diff">
<span class="sig-name descname"><span class="pre">diff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">append</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.diff" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diff()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.digamma">
<span class="sig-name descname"><span class="pre">digamma</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.digamma" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.digamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.digamma_">
<span class="sig-name descname"><span class="pre">digamma_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.digamma_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.digamma" title="unit_scaling.parameter.Tensor.digamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">digamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dim">
<span class="sig-name descname"><span class="pre">dim</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dim" title="Link to this definition"></a></dt>
<dd><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dim_order">
<span class="sig-name descname"><span class="pre">dim_order</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a></span></span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.dim_order"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.dim_order" title="Link to this definition"></a></dt>
<dd><p>Returns a tuple of int describing the dim order or physical layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None</strong></p>
</dd>
</dl>
<p>Dim order represents how dimensions are laid out in memory,
starting from the outermost to the innermost dimension.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span><span class="o">.</span><span class="n">dim_order</span><span class="p">()</span>
<span class="go">(0, 1, 2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span><span class="o">.</span><span class="n">dim_order</span><span class="p">()</span>
<span class="go">(0, 2, 3, 1)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The dim_order tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dist">
<span class="sig-name descname"><span class="pre">dist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dist" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.div">
<span class="sig-name descname"><span class="pre">div</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.div" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.div.html#torch.div" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.div_">
<span class="sig-name descname"><span class="pre">div_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.div_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.div" title="unit_scaling.parameter.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.divide">
<span class="sig-name descname"><span class="pre">divide</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.divide" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.divide()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.divide_">
<span class="sig-name descname"><span class="pre">divide_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.divide_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.divide" title="unit_scaling.parameter.Tensor.divide"><code class="xref py py-meth docutils literal notranslate"><span class="pre">divide()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dot">
<span class="sig-name descname"><span class="pre">dot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dot" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.double" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.dsplit">
<span class="sig-name descname"><span class="pre">dsplit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.dsplit" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dsplit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.element_size">
<span class="sig-name descname"><span class="pre">element_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.element_size" title="Link to this definition"></a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.eq">
<span class="sig-name descname"><span class="pre">eq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.eq" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.eq_">
<span class="sig-name descname"><span class="pre">eq_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.eq_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.eq" title="unit_scaling.parameter.Tensor.eq"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.equal">
<span class="sig-name descname"><span class="pre">equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.equal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erf">
<span class="sig-name descname"><span class="pre">erf</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erf" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erf_">
<span class="sig-name descname"><span class="pre">erf_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erf_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.erf" title="unit_scaling.parameter.Tensor.erf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erfc">
<span class="sig-name descname"><span class="pre">erfc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erfc" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erfc_">
<span class="sig-name descname"><span class="pre">erfc_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erfc_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.erfc" title="unit_scaling.parameter.Tensor.erfc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erfinv">
<span class="sig-name descname"><span class="pre">erfinv</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erfinv" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.erfinv_">
<span class="sig-name descname"><span class="pre">erfinv_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.erfinv_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.erfinv" title="unit_scaling.parameter.Tensor.erfinv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.exp">
<span class="sig-name descname"><span class="pre">exp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.exp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.exp2">
<span class="sig-name descname"><span class="pre">exp2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.exp2" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.exp2_">
<span class="sig-name descname"><span class="pre">exp2_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.exp2_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.exp2" title="unit_scaling.parameter.Tensor.exp2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.exp_">
<span class="sig-name descname"><span class="pre">exp_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.exp_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.exp" title="unit_scaling.parameter.Tensor.exp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.expand" title="Link to this definition"></a></dt>
<dd><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. For the new dimensions, the
size cannot be set to -1.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal notranslate"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>*sizes</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – the desired expanded size</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of an expanded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensors, please clone them first.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.expand_as">
<span class="sig-name descname"><span class="pre">expand_as</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.expand_as" title="Link to this definition"></a></dt>
<dd><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.expand_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.expand(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#unit_scaling.parameter.Tensor.expand" title="unit_scaling.parameter.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">expand</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.expm1">
<span class="sig-name descname"><span class="pre">expm1</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.expm1" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.expm1_">
<span class="sig-name descname"><span class="pre">expm1_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.expm1_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.expm1" title="unit_scaling.parameter.Tensor.expm1"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.exponential_">
<span class="sig-name descname"><span class="pre">exponential_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.exponential_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the PDF (probability density function):</p>
<div class="math notranslate nohighlight">
\[f(x) = \lambda e^{-\lambda x}, x &gt; 0\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In probability theory, exponential distribution is supported on interval [0, <span class="math notranslate nohighlight">\(\inf\)</span>) (i.e., <span class="math notranslate nohighlight">\(x &gt;= 0\)</span>)
implying that zero can be sampled from the exponential distribution.
However, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.exponential_()</span></code> does not sample zero,
which means that its actual support is the interval (0, <span class="math notranslate nohighlight">\(\inf\)</span>).</p>
<p>Note that <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributions.exponential.Exponential()</span></code> is supported on the interval [0, <span class="math notranslate nohighlight">\(\inf\)</span>) and can sample zero.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fill_">
<span class="sig-name descname"><span class="pre">fill_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fill_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fill_diagonal_">
<span class="sig-name descname"><span class="pre">fill_diagonal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fill_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fill_diagonal_" title="Link to this definition"></a></dt>
<dd><p>Fill the main diagonal of a tensor that has at least 2-dimensions.
When dims&gt;2, all dimensions of input must be of equal length.
This function modifies the input tensor in-place, and returns the input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fill_value</strong> (<em>Scalar</em>) – the fill value</p></li>
<li><p><strong>wrap</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – the diagonal ‘wrapped’ after N columns for tall matrices.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.],</span>
<span class="go">        [0., 0., 0.],</span>
<span class="go">        [5., 0., 0.],</span>
<span class="go">        [0., 5., 0.],</span>
<span class="go">        [0., 0., 5.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fix">
<span class="sig-name descname"><span class="pre">fix</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fix" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fix()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fix_">
<span class="sig-name descname"><span class="pre">fix_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fix_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.fix" title="unit_scaling.parameter.Tensor.fix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fix()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.flatten">
<span class="sig-name descname"><span class="pre">flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.flatten" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.flip">
<span class="sig-name descname"><span class="pre">flip</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.flip" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fliplr">
<span class="sig-name descname"><span class="pre">fliplr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fliplr" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fliplr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.flipud">
<span class="sig-name descname"><span class="pre">flipud</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.flipud" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flipud()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.float" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.float_power">
<span class="sig-name descname"><span class="pre">float_power</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.float_power" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.float_power()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.float_power_">
<span class="sig-name descname"><span class="pre">float_power_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.float_power_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.float_power" title="unit_scaling.parameter.Tensor.float_power"><code class="xref py py-meth docutils literal notranslate"><span class="pre">float_power()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.floor">
<span class="sig-name descname"><span class="pre">floor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.floor" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.floor_">
<span class="sig-name descname"><span class="pre">floor_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.floor_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.floor" title="unit_scaling.parameter.Tensor.floor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.floor_divide">
<span class="sig-name descname"><span class="pre">floor_divide</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.floor_divide" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor_divide()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.floor_divide_">
<span class="sig-name descname"><span class="pre">floor_divide_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.floor_divide_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.floor_divide" title="unit_scaling.parameter.Tensor.floor_divide"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor_divide()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fmax">
<span class="sig-name descname"><span class="pre">fmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fmax" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmax()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fmin">
<span class="sig-name descname"><span class="pre">fmin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fmin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fmod">
<span class="sig-name descname"><span class="pre">fmod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fmod" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.fmod_">
<span class="sig-name descname"><span class="pre">fmod_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.fmod_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.fmod" title="unit_scaling.parameter.Tensor.fmod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.frac">
<span class="sig-name descname"><span class="pre">frac</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.frac" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.frac_">
<span class="sig-name descname"><span class="pre">frac_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.frac_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.frac" title="unit_scaling.parameter.Tensor.frac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.frexp">
<span class="sig-name descname"><span class="pre">frexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input)</span> <span class="pre">-&gt;</span> <span class="pre">(Tensor</span> <span class="pre">mantissa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Tensor</span> <span class="pre">exponent</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.frexp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.gather">
<span class="sig-name descname"><span class="pre">gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.gather" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.gcd">
<span class="sig-name descname"><span class="pre">gcd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.gcd" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gcd()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.gcd_">
<span class="sig-name descname"><span class="pre">gcd_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.gcd_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.gcd" title="unit_scaling.parameter.Tensor.gcd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gcd()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ge">
<span class="sig-name descname"><span class="pre">ge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ge" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ge_">
<span class="sig-name descname"><span class="pre">ge_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ge_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.ge" title="unit_scaling.parameter.Tensor.ge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.geometric_">
<span class="sig-name descname"><span class="pre">geometric_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.geometric_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p>
<div class="math notranslate nohighlight">
\[P(X=k) = (1 - p)^{k - 1} p, k = 1, 2, ...\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.geometric_()</span></code> <cite>k</cite>-th trial is the first success hence draws samples in <span class="math notranslate nohighlight">\(\{1, 2, \ldots\}\)</span>, whereas
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributions.geometric.Geometric()</span></code> <span class="math notranslate nohighlight">\((k+1)\)</span>-th trial is the first success
hence draws samples in <span class="math notranslate nohighlight">\(\{0, 1, \ldots\}\)</span>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.geqrf">
<span class="sig-name descname"><span class="pre">geqrf</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.geqrf" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ger">
<span class="sig-name descname"><span class="pre">ger</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ger" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.get_device">
<span class="sig-name descname"><span class="pre">get_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">)</span> <span class="pre">-&gt;</span> <span class="pre">Device</span> <span class="pre">ordinal</span> <span class="pre">(Integer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.get_device" title="Link to this definition"></a></dt>
<dd><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
For CPU tensors, this function returns <cite>-1</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">-1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.grad">
<span class="sig-name descname"><span class="pre">grad</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.grad" title="Link to this definition"></a></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to
<a class="reference internal" href="#unit_scaling.parameter.Tensor.backward" title="unit_scaling.parameter.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.
The attribute will then contain the gradients computed and future calls to
<a class="reference internal" href="#unit_scaling.parameter.Tensor.backward" title="unit_scaling.parameter.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will accumulate (add) gradients into it.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.greater">
<span class="sig-name descname"><span class="pre">greater</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.greater" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.greater_">
<span class="sig-name descname"><span class="pre">greater_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.greater_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.greater" title="unit_scaling.parameter.Tensor.greater"><code class="xref py py-meth docutils literal notranslate"><span class="pre">greater()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.greater_equal">
<span class="sig-name descname"><span class="pre">greater_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.greater_equal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.greater_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.greater_equal_">
<span class="sig-name descname"><span class="pre">greater_equal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.greater_equal_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.greater_equal" title="unit_scaling.parameter.Tensor.greater_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">greater_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.gt">
<span class="sig-name descname"><span class="pre">gt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.gt" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.gt_">
<span class="sig-name descname"><span class="pre">gt_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.gt_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.gt" title="unit_scaling.parameter.Tensor.gt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.half" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.hardshrink">
<span class="sig-name descname"><span class="pre">hardshrink</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.hardshrink" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.hardshrink()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.has_names">
<span class="sig-name descname"><span class="pre">has_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.has_names" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if any of this tensor’s dimensions are named. Otherwise, is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.heaviside">
<span class="sig-name descname"><span class="pre">heaviside</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.heaviside" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.heaviside()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.heaviside_">
<span class="sig-name descname"><span class="pre">heaviside_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.heaviside_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.heaviside" title="unit_scaling.parameter.Tensor.heaviside"><code class="xref py py-meth docutils literal notranslate"><span class="pre">heaviside()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.histc">
<span class="sig-name descname"><span class="pre">histc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.histc" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.histogram">
<span class="sig-name descname"><span class="pre">histogram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bins</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">density</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.histogram" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histogram()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.hsplit">
<span class="sig-name descname"><span class="pre">hsplit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.hsplit" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hsplit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.hypot">
<span class="sig-name descname"><span class="pre">hypot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.hypot" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hypot()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.hypot_">
<span class="sig-name descname"><span class="pre">hypot_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.hypot_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.hypot" title="unit_scaling.parameter.Tensor.hypot"><code class="xref py py-meth docutils literal notranslate"><span class="pre">hypot()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.i0">
<span class="sig-name descname"><span class="pre">i0</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.i0" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.i0()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.i0_">
<span class="sig-name descname"><span class="pre">i0_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.i0_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.i0" title="unit_scaling.parameter.Tensor.i0"><code class="xref py py-meth docutils literal notranslate"><span class="pre">i0()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.igamma">
<span class="sig-name descname"><span class="pre">igamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.igamma" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.igamma_">
<span class="sig-name descname"><span class="pre">igamma_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.igamma_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.igamma" title="unit_scaling.parameter.Tensor.igamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">igamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.igammac">
<span class="sig-name descname"><span class="pre">igammac</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.igammac" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.igammac()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.igammac_">
<span class="sig-name descname"><span class="pre">igammac_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.igammac_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.igammac" title="unit_scaling.parameter.Tensor.igammac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">igammac()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.imag">
<span class="sig-name descname"><span class="pre">imag</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.imag" title="Link to this definition"></a></dt>
<dd><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.
The returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> share the same underlying storage.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.imag" title="unit_scaling.parameter.Tensor.imag"><code class="xref py py-func docutils literal notranslate"><span class="pre">imag()</span></code></a> is only supported for tensors with complex dtypes.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">imag</span>
<span class="go">tensor([ 0.3553, -0.7896, -0.0633, -0.8119])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_add">
<span class="sig-name descname"><span class="pre">index_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_add" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_add_">
<span class="sig-name descname"><span class="pre">index_add_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_add_" title="Link to this definition"></a></dt>
<dd><p>Accumulate the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> times <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor by adding to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example,
if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, and <code class="docutils literal notranslate"><span class="pre">alpha=-1</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of
<code class="docutils literal notranslate"><span class="pre">source</span></code> is subtracted from the <code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="docutils literal notranslate"><span class="pre">source</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<p>For a 3-D tensor the output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">src</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – indices of <code class="docutils literal notranslate"><span class="pre">source</span></code> to select from,
should have dtype either <cite>torch.int64</cite> or <cite>torch.int32</cite></p></li>
<li><p><strong>source</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the tensor containing values to add</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>alpha</strong> (<em>Number</em>) – the scalar multiplier for <code class="docutils literal notranslate"><span class="pre">source</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[  2.,   3.,   4.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  8.,   9.,  10.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  5.,   6.,   7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  1.,   1.,   1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_copy">
<span class="sig-name descname"><span class="pre">index_copy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_copy" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_copy_">
<span class="sig-name descname"><span class="pre">index_copy_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_copy_" title="Link to this definition"></a></dt>
<dd><p>Copies the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting
the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> is copied to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> contains duplicate entries, multiple elements from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> will be copied to the same index of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. The result
is nondeterministic since it depends on which copy occurs last.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> to select from</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_fill">
<span class="sig-name descname"><span class="pre">index_fill</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_fill" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_fill_">
<span class="sig-name descname"><span class="pre">index_fill_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_fill_" title="Link to this definition"></a></dt>
<dd><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> by
selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to fill in</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – the value to fill with</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[-1.,  2., -1.],</span>
<span class="go">        [-1.,  5., -1.],</span>
<span class="go">        [-1.,  8., -1.]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_put">
<span class="sig-name descname"><span class="pre">index_put</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_put" title="Link to this definition"></a></dt>
<dd><p>Out-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.index_put_" title="unit_scaling.parameter.Tensor.index_put_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_put_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_put_">
<span class="sig-name descname"><span class="pre">index_put_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_put_" title="Link to this definition"></a></dt>
<dd><p>Puts values from the tensor <a class="reference internal" href="#unit_scaling.parameter.Tensor.values" title="unit_scaling.parameter.Tensor.values"><code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code></a> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using
the indices specified in <a class="reference internal" href="#unit_scaling.parameter.Tensor.indices" title="unit_scaling.parameter.Tensor.indices"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code></a> (which is a tuple of Tensors). The
expression <code class="docutils literal notranslate"><span class="pre">tensor.index_put_(indices,</span> <span class="pre">values)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">tensor[indices]</span> <span class="pre">=</span> <span class="pre">values</span></code>. Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="#unit_scaling.parameter.Tensor.values" title="unit_scaling.parameter.Tensor.values"><code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><em>LongTensor</em>) – tensors used to index into <cite>self</cite>.</p></li>
<li><p><strong>values</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – tensor of same dtype as <cite>self</cite>.</p></li>
<li><p><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_reduce_">
<span class="sig-name descname"><span class="pre">index_reduce_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_reduce_" title="Link to this definition"></a></dt>
<dd><p>Accumulate the elements of <code class="docutils literal notranslate"><span class="pre">source</span></code> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor by accumulating to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>
using the reduction given by the <code class="docutils literal notranslate"><span class="pre">reduce</span></code> argument. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>,
<code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, <code class="docutils literal notranslate"><span class="pre">reduce</span> <span class="pre">==</span> <span class="pre">prod</span></code> and <code class="docutils literal notranslate"><span class="pre">include_self</span> <span class="pre">==</span> <span class="pre">True</span></code> then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th
row of <code class="docutils literal notranslate"><span class="pre">source</span></code> is multiplied by the <code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If
<code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=&quot;True&quot;</span></code>, the values in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor are included
in the reduction, otherwise, rows in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor that are accumulated
to are treated as if they were filled with the reduction identites.</p>
<p>The <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <code class="docutils literal notranslate"><span class="pre">source</span></code> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<p>For a 3-D tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">reduce=&quot;prod&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=True</span></code> the
output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only supports floating point tensors.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is in beta and may change in the near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – indices of <code class="docutils literal notranslate"><span class="pre">source</span></code> to select from,
should have dtype either <cite>torch.int64</cite> or <cite>torch.int32</cite></p></li>
<li><p><strong>source</strong> (<em>FloatTensor</em>) – the tensor containing values to accumulate</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the reduction operation to apply
(<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>include_self</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether the elements from the <code class="docutils literal notranslate"><span class="pre">self</span></code> tensor are
included in the reduction</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_reduce_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;prod&#39;</span><span class="p">)</span>
<span class="go">tensor([[20., 44., 72.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [14., 16., 18.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 8., 10., 12.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_reduce_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;prod&#39;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([[10., 22., 36.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.index_select">
<span class="sig-name descname"><span class="pre">index_select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.index_select" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.indices">
<span class="sig-name descname"><span class="pre">indices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.indices" title="Link to this definition"></a></dt>
<dd><p>Return the indices tensor of a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse COO tensor</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.values" title="unit_scaling.parameter.Tensor.values"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.values()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<a class="reference internal" href="#unit_scaling.parameter.Tensor.coalesce" title="unit_scaling.parameter.Tensor.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code></a> for details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.inner">
<span class="sig-name descname"><span class="pre">inner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.inner" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inner()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.int">
<span class="sig-name descname"><span class="pre">int</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.int" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.int_repr">
<span class="sig-name descname"><span class="pre">int_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.int_repr" title="Link to this definition"></a></dt>
<dd><p>Given a quantized Tensor,
<code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.inverse">
<span class="sig-name descname"><span class="pre">inverse</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.inverse" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ipu">
<span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ipu" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of this object in IPU memory.</p>
<p>If this object is already in IPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination IPU device.
Defaults to the current IPU device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_coalesced">
<span class="sig-name descname"><span class="pre">is_coalesced</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_coalesced" title="Link to this definition"></a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse COO tensor</span></a> that is coalesced, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See <a class="reference internal" href="#unit_scaling.parameter.Tensor.coalesce" title="unit_scaling.parameter.Tensor.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">coalesce()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-uncoalesced-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">uncoalesced tensors</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_complex">
<span class="sig-name descname"><span class="pre">is_complex</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_complex" title="Link to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a complex data type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_conj">
<span class="sig-name descname"><span class="pre">is_conj</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_conj" title="Link to this definition"></a></dt>
<dd><p>Returns True if the conjugate bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_contiguous">
<span class="sig-name descname"><span class="pre">is_contiguous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_contiguous" title="Link to this definition"></a></dt>
<dd><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in the order specified
by memory format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – Specifies memory allocation
order. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_cpu">
<span class="sig-name descname"><span class="pre">is_cpu</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_cpu" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the CPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_cuda">
<span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_cuda" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_floating_point">
<span class="sig-name descname"><span class="pre">is_floating_point</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_floating_point" title="Link to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_inference">
<span class="sig-name descname"><span class="pre">is_inference</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_inference" title="Link to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.is_inference()</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_ipu">
<span class="sig-name descname"><span class="pre">is_ipu</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_ipu" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the IPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_leaf">
<span class="sig-name descname"><span class="pre">is_leaf</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_leaf" title="Link to this definition"></a></dt>
<dd><p>All Tensors that have <a class="reference internal" href="#unit_scaling.parameter.Tensor.requires_grad" title="unit_scaling.parameter.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a class="reference internal" href="#unit_scaling.parameter.Tensor.requires_grad" title="unit_scaling.parameter.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">True</span></code>, they will be leaf Tensors if they were
created by the user. This means that they are not the result of an operation and so
<code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_fn</span></code> is None.</p>
<p>Only leaf Tensors will have their <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during a call to <a class="reference internal" href="#unit_scaling.parameter.Tensor.backward" title="unit_scaling.parameter.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>.
To get <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated for non-leaf Tensors, you can use <a class="reference internal" href="#unit_scaling.parameter.Tensor.retain_grad" title="unit_scaling.parameter.Tensor.retain_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain_grad()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># b was created by the operation that cast a cpu Tensor into a cuda Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># c was created by the addition operation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># e requires gradients and has no operations creating it</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># f requires grad, has no operation creating it</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_meta">
<span class="sig-name descname"><span class="pre">is_meta</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_meta" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is a meta tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.  Meta tensors
are like normal tensors, but they carry no data.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_mps">
<span class="sig-name descname"><span class="pre">is_mps</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_mps" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the MPS device, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_neg">
<span class="sig-name descname"><span class="pre">is_neg</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_neg" title="Link to this definition"></a></dt>
<dd><p>Returns True if the negative bit of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is set to true.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_pinned">
<span class="sig-name descname"><span class="pre">is_pinned</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_pinned" title="Link to this definition"></a></dt>
<dd><p>Returns true if this tensor resides in pinned memory.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_quantized">
<span class="sig-name descname"><span class="pre">is_quantized</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_quantized" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is quantized, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_set_to">
<span class="sig-name descname"><span class="pre">is_set_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_set_to" title="Link to this definition"></a></dt>
<dd><p>Returns True if both tensors are pointing to the exact same memory (same
storage, offset, size and stride).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_shared">
<span class="sig-name descname"><span class="pre">is_shared</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.is_shared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_shared" title="Link to this definition"></a></dt>
<dd><p>Checks if tensor is in shared memory.</p>
<p>This is always <code class="docutils literal notranslate"><span class="pre">True</span></code> for CUDA tensors.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_signed">
<span class="sig-name descname"><span class="pre">is_signed</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_signed" title="Link to this definition"></a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a signed data type.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_sparse">
<span class="sig-name descname"><span class="pre">is_sparse</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_sparse" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse COO storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_sparse_csr">
<span class="sig-name descname"><span class="pre">is_sparse_csr</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_sparse_csr" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor uses sparse CSR storage layout, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_xla">
<span class="sig-name descname"><span class="pre">is_xla</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_xla" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on an XLA device, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.is_xpu">
<span class="sig-name descname"><span class="pre">is_xpu</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.is_xpu" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the XPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isclose">
<span class="sig-name descname"><span class="pre">isclose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rtol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isclose" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isclose()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isfinite">
<span class="sig-name descname"><span class="pre">isfinite</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isfinite" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isfinite()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isinf">
<span class="sig-name descname"><span class="pre">isinf</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isinf" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isinf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isnan">
<span class="sig-name descname"><span class="pre">isnan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isnan" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isnan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isneginf">
<span class="sig-name descname"><span class="pre">isneginf</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isneginf" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isneginf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isposinf">
<span class="sig-name descname"><span class="pre">isposinf</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isposinf" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isposinf()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.isreal">
<span class="sig-name descname"><span class="pre">isreal</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.isreal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.isreal()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.istft">
<span class="sig-name descname"><span class="pre">istft</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_fft</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">hop_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">win_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onesided</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_complex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.istft"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.istft" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.istft()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.item">
<span class="sig-name descname"><span class="pre">item</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">number</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.item" title="Link to this definition"></a></dt>
<dd><p>Returns the value of this tensor as a standard Python number. This only works
for tensors with one element. For other cases, see <a class="reference internal" href="#unit_scaling.parameter.Tensor.tolist" title="unit_scaling.parameter.Tensor.tolist"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tolist()</span></code></a>.</p>
<p>This operation is not differentiable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.itemsize">
<span class="sig-name descname"><span class="pre">itemsize</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.itemsize" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.element_size" title="unit_scaling.parameter.Tensor.element_size"><code class="xref py py-meth docutils literal notranslate"><span class="pre">element_size()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.kron">
<span class="sig-name descname"><span class="pre">kron</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.kron" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kron()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.kthvalue">
<span class="sig-name descname"><span class="pre">kthvalue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.kthvalue" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lcm">
<span class="sig-name descname"><span class="pre">lcm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lcm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lcm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lcm_">
<span class="sig-name descname"><span class="pre">lcm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lcm_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.lcm" title="unit_scaling.parameter.Tensor.lcm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lcm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ldexp">
<span class="sig-name descname"><span class="pre">ldexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ldexp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ldexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ldexp_">
<span class="sig-name descname"><span class="pre">ldexp_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ldexp_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.ldexp" title="unit_scaling.parameter.Tensor.ldexp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ldexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.le">
<span class="sig-name descname"><span class="pre">le</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.le" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.le.html#torch.le" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.le_">
<span class="sig-name descname"><span class="pre">le_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.le_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.le" title="unit_scaling.parameter.Tensor.le"><code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lerp">
<span class="sig-name descname"><span class="pre">lerp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lerp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lerp_">
<span class="sig-name descname"><span class="pre">lerp_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lerp_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.lerp" title="unit_scaling.parameter.Tensor.lerp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.less">
<span class="sig-name descname"><span class="pre">less</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.less" title="Link to this definition"></a></dt>
<dd><p>lt(other) -&gt; Tensor</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.less.html#torch.less" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.less_">
<span class="sig-name descname"><span class="pre">less_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.less_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.less" title="unit_scaling.parameter.Tensor.less"><code class="xref py py-meth docutils literal notranslate"><span class="pre">less()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.less_equal">
<span class="sig-name descname"><span class="pre">less_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.less_equal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.less_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.less_equal_">
<span class="sig-name descname"><span class="pre">less_equal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.less_equal_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.less_equal" title="unit_scaling.parameter.Tensor.less_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">less_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lgamma">
<span class="sig-name descname"><span class="pre">lgamma</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lgamma" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lgamma_">
<span class="sig-name descname"><span class="pre">lgamma_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lgamma_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.lgamma" title="unit_scaling.parameter.Tensor.lgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lgamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.log.html#torch.log" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log10">
<span class="sig-name descname"><span class="pre">log10</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log10" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log10_">
<span class="sig-name descname"><span class="pre">log10_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log10_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.log10" title="unit_scaling.parameter.Tensor.log10"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log1p">
<span class="sig-name descname"><span class="pre">log1p</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log1p" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log1p_">
<span class="sig-name descname"><span class="pre">log1p_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log1p_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.log1p" title="unit_scaling.parameter.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log2">
<span class="sig-name descname"><span class="pre">log2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log2" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log2_">
<span class="sig-name descname"><span class="pre">log2_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log2_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.log2" title="unit_scaling.parameter.Tensor.log2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log_">
<span class="sig-name descname"><span class="pre">log_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.log" title="unit_scaling.parameter.Tensor.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.log_normal_">
<span class="sig-name descname"><span class="pre">log_normal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.log_normal_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution
parameterized by the given mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation
<span class="math notranslate nohighlight">\(\sigma\)</span>. Note that <a class="reference internal" href="#unit_scaling.parameter.Tensor.mean" title="unit_scaling.parameter.Tensor.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#unit_scaling.parameter.Tensor.std" title="unit_scaling.parameter.Tensor.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> are the mean and
standard deviation of the underlying normal distribution, and not of the
returned distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logaddexp">
<span class="sig-name descname"><span class="pre">logaddexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logaddexp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logaddexp2">
<span class="sig-name descname"><span class="pre">logaddexp2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logaddexp2" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logaddexp2()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logcumsumexp">
<span class="sig-name descname"><span class="pre">logcumsumexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logcumsumexp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logcumsumexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logdet">
<span class="sig-name descname"><span class="pre">logdet</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logdet" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_and">
<span class="sig-name descname"><span class="pre">logical_and</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_and" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_and()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_and_">
<span class="sig-name descname"><span class="pre">logical_and_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_and_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.logical_and" title="unit_scaling.parameter.Tensor.logical_and"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_and()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_not">
<span class="sig-name descname"><span class="pre">logical_not</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_not" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_not()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_not_">
<span class="sig-name descname"><span class="pre">logical_not_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_not_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.logical_not" title="unit_scaling.parameter.Tensor.logical_not"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_not()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_or">
<span class="sig-name descname"><span class="pre">logical_or</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_or" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_or()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_or_">
<span class="sig-name descname"><span class="pre">logical_or_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_or_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.logical_or" title="unit_scaling.parameter.Tensor.logical_or"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_or()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_xor">
<span class="sig-name descname"><span class="pre">logical_xor</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_xor" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logical_xor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logical_xor_">
<span class="sig-name descname"><span class="pre">logical_xor_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logical_xor_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.logical_xor" title="unit_scaling.parameter.Tensor.logical_xor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logical_xor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logit">
<span class="sig-name descname"><span class="pre">logit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logit" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logit_">
<span class="sig-name descname"><span class="pre">logit_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logit_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.logit" title="unit_scaling.parameter.Tensor.logit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.logsumexp">
<span class="sig-name descname"><span class="pre">logsumexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.logsumexp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.long">
<span class="sig-name descname"><span class="pre">long</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.long" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lt">
<span class="sig-name descname"><span class="pre">lt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lt" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lt_">
<span class="sig-name descname"><span class="pre">lt_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lt_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.lt" title="unit_scaling.parameter.Tensor.lt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lu">
<span class="sig-name descname"><span class="pre">lu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pivot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_infos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.lu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.lu" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.lu_solve">
<span class="sig-name descname"><span class="pre">lu_solve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">LU_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">LU_pivots</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.lu_solve" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lu_solve()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mH">
<span class="sig-name descname"><span class="pre">mH</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mH" title="Link to this definition"></a></dt>
<dd><p>Accessing this property is equivalent to calling <a class="reference internal" href="#unit_scaling.parameter.Tensor.adjoint" title="unit_scaling.parameter.Tensor.adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjoint()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mT">
<span class="sig-name descname"><span class="pre">mT</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mT" title="Link to this definition"></a></dt>
<dd><p>Returns a view of this tensor with the last two dimensions transposed.</p>
<p><code class="docutils literal notranslate"><span class="pre">x.mT</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.transpose(-2,</span> <span class="pre">-1)</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.map_">
<span class="sig-name descname"><span class="pre">map_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.map_" title="Link to this definition"></a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and
the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must be <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcastable</span></a>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.masked_fill">
<span class="sig-name descname"><span class="pre">masked_fill</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.masked_fill" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.masked_fill_">
<span class="sig-name descname"><span class="pre">masked_fill_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.masked_fill_" title="Link to this definition"></a></dt>
<dd><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is
True. The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be
<a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<em>BoolTensor</em>) – the boolean mask</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – the value to fill in with</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.masked_scatter">
<span class="sig-name descname"><span class="pre">masked_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.masked_scatter" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The inputs <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
<a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcast</span></a>.</p>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="go">tensor([[0, 0, 0, 0, 1],</span>
<span class="go">        [2, 3, 0, 4, 5]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.masked_scatter_">
<span class="sig-name descname"><span class="pre">masked_scatter_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.masked_scatter_" title="Link to this definition"></a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is True. Elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> are copied into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
starting at position 0 of <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> and continuing in order one-by-one for each
occurrence of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> being True.
The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" title="(in PyTorch v2.5)"><span class="xref std std-ref">broadcastable</span></a>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<em>BoolTensor</em>) – the boolean mask</p></li>
<li><p><strong>source</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the tensor to copy from</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> tensor.</p>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">masked_scatter_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="go">tensor([[0, 0, 0, 0, 1],</span>
<span class="go">        [2, 3, 0, 4, 5]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.masked_select">
<span class="sig-name descname"><span class="pre">masked_select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.masked_select" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.matmul">
<span class="sig-name descname"><span class="pre">matmul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.matmul" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.matrix_exp">
<span class="sig-name descname"><span class="pre">matrix_exp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.matrix_exp" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_exp()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.matrix_power">
<span class="sig-name descname"><span class="pre">matrix_power</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.matrix_power" title="Link to this definition"></a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.matrix_power" title="unit_scaling.parameter.Tensor.matrix_power"><code class="xref py py-meth docutils literal notranslate"><span class="pre">matrix_power()</span></code></a> is deprecated, use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code></a> instead.</p>
</div>
<p>Alias for <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.max">
<span class="sig-name descname"><span class="pre">max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.max" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.maximum">
<span class="sig-name descname"><span class="pre">maximum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.maximum" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.maximum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mean">
<span class="sig-name descname"><span class="pre">mean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mean" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.median">
<span class="sig-name descname"><span class="pre">median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.median" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.median.html#torch.median" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.min">
<span class="sig-name descname"><span class="pre">min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.min" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.min.html#torch.min" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.minimum">
<span class="sig-name descname"><span class="pre">minimum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.minimum" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.minimum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mm">
<span class="sig-name descname"><span class="pre">mm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mode" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.module_load">
<span class="sig-name descname"><span class="pre">module_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assign</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.module_load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.module_load" title="Link to this definition"></a></dt>
<dd><p>Defines how to transform <code class="docutils literal notranslate"><span class="pre">other</span></code> when loading it into <code class="docutils literal notranslate"><span class="pre">self</span></code> in <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code>.</p>
<p>Used when <a class="reference external" href="https://pytorch.org/docs/stable/future_mod.html#torch.__future__.get_swap_module_params_on_conversion" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_swap_module_params_on_conversion()</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>It is expected that <code class="docutils literal notranslate"><span class="pre">self</span></code> is a parameter or buffer in an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and <code class="docutils literal notranslate"><span class="pre">other</span></code> is the
value in the state dictionary with the corresponding key, this method defines
how <code class="docutils literal notranslate"><span class="pre">other</span></code> is remapped before being swapped with <code class="docutils literal notranslate"><span class="pre">self</span></code> via
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.utils.swap_tensors.html#torch.utils.swap_tensors" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">swap_tensors()</span></code></a> in <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method should always return a new object that is not <code class="docutils literal notranslate"><span class="pre">self</span></code> or <code class="docutils literal notranslate"><span class="pre">other</span></code>.
For example, the default implementation returns <code class="docutils literal notranslate"><span class="pre">self.copy_(other).detach()</span></code>
if <code class="docutils literal notranslate"><span class="pre">assign</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">other.detach()</span></code> if <code class="docutils literal notranslate"><span class="pre">assign</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>other</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – value in state dict with key corresponding to <code class="docutils literal notranslate"><span class="pre">self</span></code></p></li>
<li><p><strong>assign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – the assign argument passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">nn.Module.load_state_dict()</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.moveaxis">
<span class="sig-name descname"><span class="pre">moveaxis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.moveaxis" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.moveaxis()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.movedim">
<span class="sig-name descname"><span class="pre">movedim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.movedim" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.msort">
<span class="sig-name descname"><span class="pre">msort</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.msort" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.msort()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mtia">
<span class="sig-name descname"><span class="pre">mtia</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mtia" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of this object in MTIA memory.</p>
<p>If this object is already in MTIA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination MTIA device.
Defaults to the current MTIA device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mul">
<span class="sig-name descname"><span class="pre">mul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mul" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mul_">
<span class="sig-name descname"><span class="pre">mul_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mul_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.mul" title="unit_scaling.parameter.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.multinomial">
<span class="sig-name descname"><span class="pre">multinomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replacement</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.multinomial" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.multiply">
<span class="sig-name descname"><span class="pre">multiply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.multiply" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multiply()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.multiply_">
<span class="sig-name descname"><span class="pre">multiply_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.multiply_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.multiply" title="unit_scaling.parameter.Tensor.multiply"><code class="xref py py-meth docutils literal notranslate"><span class="pre">multiply()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mv">
<span class="sig-name descname"><span class="pre">mv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mv" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mvlgamma">
<span class="sig-name descname"><span class="pre">mvlgamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mvlgamma" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.mvlgamma_">
<span class="sig-name descname"><span class="pre">mvlgamma_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.mvlgamma_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.mvlgamma" title="unit_scaling.parameter.Tensor.mvlgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.names">
<span class="sig-name descname"><span class="pre">names</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.names" title="Link to this definition"></a></dt>
<dd><p>Stores names for each of this tensor’s dimensions.</p>
<p><code class="docutils literal notranslate"><span class="pre">names[idx]</span></code> corresponds to the name of tensor dimension <code class="docutils literal notranslate"><span class="pre">idx</span></code>.
Names are either a string if the dimension is named or <code class="docutils literal notranslate"><span class="pre">None</span></code> if the
dimension is unnamed.</p>
<p>Dimension names may contain characters or underscore. Furthermore, a dimension
name must be a valid Python variable name (i.e., does not start with underscore).</p>
<p>Tensors may not have two named dimensions with the same name.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nan_to_num">
<span class="sig-name descname"><span class="pre">nan_to_num</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posinf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neginf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nan_to_num" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nan_to_num()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nan_to_num_">
<span class="sig-name descname"><span class="pre">nan_to_num_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nan</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posinf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neginf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nan_to_num_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.nan_to_num" title="unit_scaling.parameter.Tensor.nan_to_num"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nan_to_num()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nanmean">
<span class="sig-name descname"><span class="pre">nanmean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nanmean" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmean()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nanmedian">
<span class="sig-name descname"><span class="pre">nanmedian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nanmedian" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanmedian()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nanquantile">
<span class="sig-name descname"><span class="pre">nanquantile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interpolation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nanquantile" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nanquantile()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nansum">
<span class="sig-name descname"><span class="pre">nansum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nansum" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nansum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.narrow">
<span class="sig-name descname"><span class="pre">narrow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.narrow" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.narrow_copy">
<span class="sig-name descname"><span class="pre">narrow_copy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.narrow_copy" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.narrow_copy.html#torch.narrow_copy" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow_copy()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nbytes">
<span class="sig-name descname"><span class="pre">nbytes</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nbytes" title="Link to this definition"></a></dt>
<dd><p>Returns the number of bytes consumed by the “view” of elements of the Tensor
if the Tensor does not use sparse storage layout.
Defined to be <a class="reference internal" href="#unit_scaling.parameter.Tensor.numel" title="unit_scaling.parameter.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a> * <a class="reference internal" href="#unit_scaling.parameter.Tensor.element_size" title="unit_scaling.parameter.Tensor.element_size"><code class="xref py py-meth docutils literal notranslate"><span class="pre">element_size()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ndim">
<span class="sig-name descname"><span class="pre">ndim</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ndim" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ndimension">
<span class="sig-name descname"><span class="pre">ndimension</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ndimension" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ne">
<span class="sig-name descname"><span class="pre">ne</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ne" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ne_">
<span class="sig-name descname"><span class="pre">ne_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ne_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.ne" title="unit_scaling.parameter.Tensor.ne"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.neg">
<span class="sig-name descname"><span class="pre">neg</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.neg" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.neg_">
<span class="sig-name descname"><span class="pre">neg_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.neg_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.neg" title="unit_scaling.parameter.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.negative">
<span class="sig-name descname"><span class="pre">negative</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.negative" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.negative()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.negative_">
<span class="sig-name descname"><span class="pre">negative_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.negative_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.negative" title="unit_scaling.parameter.Tensor.negative"><code class="xref py py-meth docutils literal notranslate"><span class="pre">negative()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nelement">
<span class="sig-name descname"><span class="pre">nelement</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nelement" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.numel" title="unit_scaling.parameter.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_empty">
<span class="sig-name descname"><span class="pre">new_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_empty" title="Link to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.
By default, the returned Tensor has the same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a list, tuple, or <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a> of integers defining the
shape of the output tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_empty_strided">
<span class="sig-name descname"><span class="pre">new_empty_strided</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_empty_strided" title="Link to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> and strides <a class="reference internal" href="#unit_scaling.parameter.Tensor.stride" title="unit_scaling.parameter.Tensor.stride"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code></a> filled with
uninitialized data. By default, the returned Tensor has the same
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a list, tuple, or <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a> of integers defining the
shape of the output tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty_strided</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_full">
<span class="sig-name descname"><span class="pre">new_full</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_full" title="Link to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
By default, the returned Tensor has the same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>fill_value</strong> (<em>scalar</em>) – the number to fill the output tensor with.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_ones">
<span class="sig-name descname"><span class="pre">new_ones</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_ones" title="Link to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.
By default, the returned Tensor has the same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a list, tuple, or <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a> of integers defining the
shape of the output tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 1,  1,  1],</span>
<span class="go">        [ 1,  1,  1]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_tensor">
<span class="sig-name descname"><span class="pre">new_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_tensor" title="Link to this definition"></a></dt>
<dd><p>Returns a new Tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> as the tensor data.
By default, the returned Tensor has the same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.new_tensor" title="unit_scaling.parameter.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code>
or <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code>.
If you have a numpy array and want to avoid a copy, use
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <a class="reference internal" href="#unit_scaling.parameter.Tensor.new_tensor" title="unit_scaling.parameter.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>array_like</em>) – The returned Tensor copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 2,  3]], dtype=torch.int8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.new_zeros">
<span class="sig-name descname"><span class="pre">new_zeros</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.new_zeros" title="Link to this definition"></a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.
By default, the returned Tensor has the same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a list, tuple, or <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a> of integers defining the
shape of the output tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nextafter">
<span class="sig-name descname"><span class="pre">nextafter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nextafter" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nextafter()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nextafter_">
<span class="sig-name descname"><span class="pre">nextafter_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nextafter_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.nextafter" title="unit_scaling.parameter.Tensor.nextafter"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nextafter()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nonzero">
<span class="sig-name descname"><span class="pre">nonzero</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nonzero" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.nonzero_static">
<span class="sig-name descname"><span class="pre">nonzero_static</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fill_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.nonzero_static" title="Link to this definition"></a></dt>
<dd><p>Returns a 2-D tensor where each row is the index for a non-zero value.
The returned Tensor has the same <cite>torch.dtype</cite> as <cite>torch.nonzero()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the input tensor to count non-zero elements.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the size of non-zero elements expected to be included in the out
tensor. Pad the out tensor with <cite>fill_value</cite> if the <cite>size</cite> is larger
than total number of non-zero elements, truncate out tensor if <cite>size</cite>
is smaller. The size must be a non-negative integer.</p></li>
<li><p><strong>fill_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the value to fill the output tensor with when <cite>size</cite> is larger
than the total number of non-zero elements. Default is <cite>-1</cite> to represent
invalid index.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p># Example 1: Padding
&gt;&gt;&gt; input_tensor = torch.tensor([[1, 0], [3, 2]])
&gt;&gt;&gt; static_size = 4
&gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size = static_size)
tensor([[  0,   0],</p>
<blockquote>
<div><p>[  1,   0],
[  1,   1],
[  -1, -1]], dtype=torch.int64)</p>
</div></blockquote>
<p># Example 2: Truncating
&gt;&gt;&gt; input_tensor = torch.tensor([[1, 0], [3, 2]])
&gt;&gt;&gt; static_size = 2
&gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size = static_size)
tensor([[  0,   0],</p>
<blockquote>
<div><p>[  1,   0]], dtype=torch.int64)</p>
</div></blockquote>
<p># Example 3: 0 size
&gt;&gt;&gt; input_tensor = torch.tensor([10])
&gt;&gt;&gt; static_size = 0
&gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size = static_size)
tensor([], size=(0, 1), dtype=torch.int64)</p>
<p># Example 4: 0 rank input
&gt;&gt;&gt; input_tensor = torch.tensor(10)
&gt;&gt;&gt; static_size = 2
&gt;&gt;&gt; t = torch.nonzero_static(input_tensor, size = static_size)
tensor([], size=(2, 0), dtype=torch.int64)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.norm">
<span class="sig-name descname"><span class="pre">norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'fro'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.norm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.normal_">
<span class="sig-name descname"><span class="pre">normal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.normal_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="#unit_scaling.parameter.Tensor.mean" title="unit_scaling.parameter.Tensor.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#unit_scaling.parameter.Tensor.std" title="unit_scaling.parameter.Tensor.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.not_equal">
<span class="sig-name descname"><span class="pre">not_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.not_equal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.not_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.not_equal_">
<span class="sig-name descname"><span class="pre">not_equal_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.not_equal_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.not_equal" title="unit_scaling.parameter.Tensor.not_equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">not_equal()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.numel">
<span class="sig-name descname"><span class="pre">numel</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.numel" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.numpy">
<span class="sig-name descname"><span class="pre">numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">numpy.ndarray</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.numpy" title="Link to this definition"></a></dt>
<dd><p>Returns the tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> (the default), the conversion
is performed only if the tensor is on the CPU, does not require grad,
does not have its conjugate bit set, and is a dtype and layout that
NumPy supports. The returned ndarray and the tensor will share their
storage, so changes to the tensor will be reflected in the ndarray
and vice versa.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this is equivalent to
calling <code class="docutils literal notranslate"><span class="pre">t.detach().cpu().resolve_conj().resolve_neg().numpy()</span></code>.
If the tensor isn’t on the CPU or the conjugate or negative bit is set,
the tensor won’t share its storage with the returned ndarray.
Setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">force</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> can be a useful shorthand.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>force</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the ndarray may be a copy of the tensor
instead of always sharing memory, defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.orgqr">
<span class="sig-name descname"><span class="pre">orgqr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.orgqr" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ormqr">
<span class="sig-name descname"><span class="pre">ormqr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transpose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ormqr" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.outer">
<span class="sig-name descname"><span class="pre">outer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.outer" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.permute">
<span class="sig-name descname"><span class="pre">permute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.permute" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.permute()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.pin_memory">
<span class="sig-name descname"><span class="pre">pin_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.pin_memory" title="Link to this definition"></a></dt>
<dd><p>Copies the tensor to pinned memory, if it’s not already pinned.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.pinverse">
<span class="sig-name descname"><span class="pre">pinverse</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.pinverse" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.polygamma">
<span class="sig-name descname"><span class="pre">polygamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.polygamma" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.polygamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.polygamma_">
<span class="sig-name descname"><span class="pre">polygamma_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.polygamma_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.polygamma" title="unit_scaling.parameter.Tensor.polygamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">polygamma()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.positive">
<span class="sig-name descname"><span class="pre">positive</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.positive" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.positive()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.pow">
<span class="sig-name descname"><span class="pre">pow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.pow" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.pow_">
<span class="sig-name descname"><span class="pre">pow_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exponent</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.pow_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.pow" title="unit_scaling.parameter.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.prod">
<span class="sig-name descname"><span class="pre">prod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.prod" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.put">
<span class="sig-name descname"><span class="pre">put</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.put" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.put_()</span></code></a>.
<cite>input</cite> corresponds to <cite>self</cite> in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.put_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.put_">
<span class="sig-name descname"><span class="pre">put_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.put_" title="Link to this definition"></a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into the positions specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For the purpose of indexing, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is treated as if
it were a 1-D tensor.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> need to have the same number of elements, but not necessarily
the same shape.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices into self</p></li>
<li><p><strong>source</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy from</p></li>
<li><p><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">tensor([[  4,   9,   5],</span>
<span class="go">        [ 10,   7,   8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.q_per_channel_axis">
<span class="sig-name descname"><span class="pre">q_per_channel_axis</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.q_per_channel_axis" title="Link to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns the index of dimension on which per-channel quantization is applied.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.q_per_channel_scales">
<span class="sig-name descname"><span class="pre">q_per_channel_scales</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.q_per_channel_scales" title="Link to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a Tensor of scales of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.q_per_channel_zero_points">
<span class="sig-name descname"><span class="pre">q_per_channel_zero_points</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.q_per_channel_zero_points" title="Link to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear (affine) per-channel quantization,
returns a tensor of zero_points of the underlying quantizer. It has the number of
elements that matches the corresponding dimensions (from q_per_channel_axis) of
the tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.q_scale">
<span class="sig-name descname"><span class="pre">q_scale</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.q_scale" title="Link to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the scale of the underlying quantizer().</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.q_zero_point">
<span class="sig-name descname"><span class="pre">q_zero_point</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.q_zero_point" title="Link to this definition"></a></dt>
<dd><p>Given a Tensor quantized by linear(affine) quantization,
returns the zero_point of the underlying quantizer().</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.qr">
<span class="sig-name descname"><span class="pre">qr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">some</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.qr" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.qscheme">
<span class="sig-name descname"><span class="pre">qscheme</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.qscheme</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.qscheme" title="Link to this definition"></a></dt>
<dd><p>Returns the quantization scheme of a given QTensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.quantile">
<span class="sig-name descname"><span class="pre">quantile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interpolation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.quantile" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rad2deg">
<span class="sig-name descname"><span class="pre">rad2deg</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.rad2deg" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rad2deg()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rad2deg_">
<span class="sig-name descname"><span class="pre">rad2deg_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.rad2deg_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.rad2deg" title="unit_scaling.parameter.Tensor.rad2deg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rad2deg()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.random_">
<span class="sig-name descname"><span class="pre">random_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">from=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator=None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.random_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform
distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If not specified, the values are usually
only bounded by <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s data type. However, for floating point
types, if unspecified, range will be <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^mantissa]</span></code> to ensure that every
value is representable. For example, <cite>torch.tensor(1, dtype=torch.double).random_()</cite>
will be uniform in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^53]</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.ravel">
<span class="sig-name descname"><span class="pre">ravel</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.ravel" title="Link to this definition"></a></dt>
<dd><p>see <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ravel()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.real">
<span class="sig-name descname"><span class="pre">real</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.real" title="Link to this definition"></a></dt>
<dd><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor for a complex-valued input tensor.
The returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> share the same underlying storage.</p>
<p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a real-valued tensor tensor.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">real</span>
<span class="go">tensor([ 0.3100, -0.5445, -1.6492, -0.0638])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.reciprocal">
<span class="sig-name descname"><span class="pre">reciprocal</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.reciprocal" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.reciprocal_">
<span class="sig-name descname"><span class="pre">reciprocal_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.reciprocal_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.reciprocal" title="unit_scaling.parameter.Tensor.reciprocal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.record_stream">
<span class="sig-name descname"><span class="pre">record_stream</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.record_stream" title="Link to this definition"></a></dt>
<dd><p>Marks the tensor as having been used by this stream.  When the tensor
is deallocated, ensure the tensor memory is not reused for another tensor
until all work queued on <code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code> at the time of deallocation is
complete.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The caching allocator is aware of only the stream where a tensor was
allocated. Due to the awareness, it already correctly manages the life
cycle of tensors on only one stream. But if a tensor is used on a stream
different from the stream of origin, the allocator might reuse the memory
unexpectedly. Calling this method lets the allocator know which streams
have used the tensor.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is most suitable for use cases where you are providing a
function that created a tensor on a side stream, and want users to be able
to make use of the tensor without having to think carefully about stream
safety when making use of them.  These safety guarantees come at some
performance and predictability cost (analogous to the tradeoff between GC
and manual memory management), so if you are in a situation where
you manage the full lifetime of your tensors, you may consider instead
manually managing CUDA events so that calling this method is not necessary.
In particular, when you call this method, on later allocations the
allocator will poll the recorded stream to see if all operations have
completed yet; you can potentially race with side stream computation and
non-deterministically reuse or fail to reuse memory for an allocation.</p>
<p>You can safely use tensors allocated on side streams without
<a class="reference internal" href="#unit_scaling.parameter.Tensor.record_stream" title="unit_scaling.parameter.Tensor.record_stream"><code class="xref py py-meth docutils literal notranslate"><span class="pre">record_stream()</span></code></a>; you must manually ensure that
any non-creation stream uses of a tensor are synced back to the creation
stream before you deallocate the tensor.  As the CUDA caching allocator
guarantees that the memory will only be reused with the same creation stream,
this is sufficient to ensure that writes to future reallocations of the
memory will be delayed until non-creation stream uses are done.
(Counterintuitively, you may observe that on the CPU side we have already
reallocated the tensor, even though CUDA kernels on the old tensor are
still in progress.  This is fine, because CUDA operations on the new
tensor will appropriately wait for the old operations to complete, as they
are all on the same stream.)</p>
<p>Concretely, this looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">s0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">s1</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">s1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">some_comm_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="o">...</span> <span class="n">some</span> <span class="n">compute</span> <span class="n">on</span> <span class="n">s0</span> <span class="o">...</span>

<span class="c1"># synchronize creation stream s0 to side stream s1</span>
<span class="c1"># before deallocating x</span>
<span class="n">s0</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
<span class="k">del</span> <span class="n">x</span>
</pre></div>
</div>
<p>Note that some discretion is required when deciding when to perform
<code class="docutils literal notranslate"><span class="pre">s0.wait_stream(s1)</span></code>.  In particular, if we were to wait immediately
after <code class="docutils literal notranslate"><span class="pre">some_comm_op</span></code>, there wouldn’t be any point in having the side
stream; it would be equivalent to have run <code class="docutils literal notranslate"><span class="pre">some_comm_op</span></code> on <code class="docutils literal notranslate"><span class="pre">s0</span></code>.
Instead, the synchronization must be placed at some appropriate, later
point in time where you expect the side stream <code class="docutils literal notranslate"><span class="pre">s1</span></code> to have finished
work.  This location is typically identified via profiling, e.g., using
Chrome traces produced
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.profiler.profile.export_chrome_trace()</span></code></a>.  If you
place the wait too early, work on s0 will block until <code class="docutils literal notranslate"><span class="pre">s1</span></code> has finished,
preventing further overlapping of communication and computation.  If you
place the wait too late, you will use more memory than is strictly
necessary (as you are keeping <code class="docutils literal notranslate"><span class="pre">x</span></code> live for longer.)  For a concrete
example of how this guidance can be applied in practice, see this post:
<a class="reference external" href="https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486">FSDP and CUDACachingAllocator</a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.refine_names">
<span class="sig-name descname"><span class="pre">refine_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.refine_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.refine_names" title="Link to this definition"></a></dt>
<dd><p>Refines the dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> according to <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.</p>
<p>Refining is a special case of renaming that “lifts” unnamed dimensions.
A <code class="docutils literal notranslate"><span class="pre">None</span></code> dim can be refined to have any name; a named dim can only be
refined to have the same name.</p>
<p>Because named tensors can coexist with unnamed tensors, refining names
gives a nice way to write named-tensor-aware code that works with both
named and unnamed tensors.</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> may contain up to one Ellipsis (<code class="docutils literal notranslate"><span class="pre">...</span></code>).
The Ellipsis is expanded greedily; it is expanded in-place to fill
<a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> to the same length as <code class="docutils literal notranslate"><span class="pre">self.dim()</span></code> using names from the
corresponding indices of <code class="docutils literal notranslate"><span class="pre">self.names</span></code>.</p>
<p>Python 2 does not support Ellipsis but one may use a string literal
instead (<code class="docutils literal notranslate"><span class="pre">'...'</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>names</strong> (<em>iterable</em><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The desired names of the output tensor. May
contain up to one Ellipsis.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">named_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.register_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.register_hook" title="Link to this definition"></a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
Tensor is computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution" title="(in PyTorch v2.5)"><span>Backward Hooks execution</span></a> for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span>

<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 6</span>
<span class="go">[torch.FloatTensor of size (3,)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.register_post_accumulate_grad_hook">
<span class="sig-name descname"><span class="pre">register_post_accumulate_grad_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.register_post_accumulate_grad_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.register_post_accumulate_grad_hook" title="Link to this definition"></a></dt>
<dd><p>Registers a backward hook that runs after grad accumulation.</p>
<p>The hook will be called after all gradients for a tensor have been accumulated,
meaning that the .grad field has been updated on that tensor. The post
accumulate grad hook is ONLY applicable for leaf tensors (tensors without a
.grad_fn field). Registering this hook on a non-leaf tensor will error!</p>
<p>The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Note that, unlike other autograd hooks, this hook operates on the tensor
that requires grad and not the grad itself. The hook can in-place modify
and access its Tensor argument, including its .grad field.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution" title="(in PyTorch v2.5)"><span>Backward Hooks execution</span></a> for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks. Since
this hook runs during the backward pass, it will run in no_grad mode (unless
create_graph is True). You can use torch.enable_grad() to re-enable autograd
within the hook if you need it.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># simulate a simple SGD update</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_post_accumulate_grad_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">lr</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>
<span class="go">tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.remainder">
<span class="sig-name descname"><span class="pre">remainder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.remainder" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.remainder_">
<span class="sig-name descname"><span class="pre">remainder_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.remainder_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.remainder" title="unit_scaling.parameter.Tensor.remainder"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rename">
<span class="sig-name descname"><span class="pre">rename</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">rename_map</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.rename"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.rename" title="Link to this definition"></a></dt>
<dd><p>Renames dimension names of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>There are two main usages:</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(**rename_map)</span></code> returns a view on tensor that has dims
renamed as specified in the mapping <code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">self.rename(*names)</span></code> returns a view on tensor, renaming all
dimensions positionally using <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a>.
Use <code class="docutils literal notranslate"><span class="pre">self.rename(None)</span></code> to drop names on a tensor.</p>
<p>One cannot specify both positional args <a class="reference internal" href="#unit_scaling.parameter.Tensor.names" title="unit_scaling.parameter.Tensor.names"><code class="xref py py-attr docutils literal notranslate"><span class="pre">names</span></code></a> and keyword args
<code class="xref py py-attr docutils literal notranslate"><span class="pre">rename_map</span></code>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="s1">&#39;channels&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(None, None, None, None)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;channel&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">renamed_imgs</span><span class="o">.</span><span class="n">names</span>
<span class="go">(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The named tensor API is experimental and subject to change.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rename_">
<span class="sig-name descname"><span class="pre">rename_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">rename_map</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.rename_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.rename_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.rename" title="unit_scaling.parameter.Tensor.rename"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rename()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.renorm">
<span class="sig-name descname"><span class="pre">renorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxnorm</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.renorm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.renorm_">
<span class="sig-name descname"><span class="pre">renorm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxnorm</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.renorm_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.renorm" title="unit_scaling.parameter.Tensor.renorm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.repeat">
<span class="sig-name descname"><span class="pre">repeat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">repeats</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.repeat" title="Link to this definition"></a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#unit_scaling.parameter.Tensor.expand" title="unit_scaling.parameter.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a>, this function copies the tensor’s data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.repeat" title="unit_scaling.parameter.Tensor.repeat"><code class="xref py py-meth docutils literal notranslate"><span class="pre">repeat()</span></code></a> behaves differently from
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html">numpy.repeat</a>,
but is more similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html">numpy.tile</a>.
For the operator similar to <cite>numpy.repeat</cite>, see <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>repeat</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of times to repeat this tensor along each dimension</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.repeat_interleave">
<span class="sig-name descname"><span class="pre">repeat_interleave</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repeats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.repeat_interleave" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat_interleave()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.requires_grad" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fact that gradients need to be computed for a Tensor do not mean that the <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>
attribute will be populated, see <a class="reference internal" href="#unit_scaling.parameter.Tensor.is_leaf" title="unit_scaling.parameter.Tensor.is_leaf"><code class="xref py py-attr docutils literal notranslate"><span class="pre">is_leaf</span></code></a> for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.requires_grad_" title="Link to this definition"></a></dt>
<dd><p>Change if autograd should record operations on this tensor: sets this tensor’s
<a class="reference internal" href="#unit_scaling.parameter.Tensor.requires_grad" title="unit_scaling.parameter.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute in-place. Returns this tensor.</p>
<p><a class="reference internal" href="#unit_scaling.parameter.Tensor.requires_grad_" title="unit_scaling.parameter.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a>’s main use case is to tell autograd to begin recording
operations on a Tensor <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. If <code class="docutils literal notranslate"><span class="pre">tensor</span></code> has <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>
(because it was obtained through a DataLoader, or required preprocessing or
initialization), <code class="docutils literal notranslate"><span class="pre">tensor.requires_grad_()</span></code> makes it so that autograd will
begin to record operations on <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If autograd should record operations on this tensor.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Let&#39;s say we want to preprocess some saved weights and use</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the result as new weights.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span>  <span class="c1"># some function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span>
<span class="go">tensor([-0.5503,  0.4926, -2.1158, -0.8303])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now, start to record operations done to weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([-1.1007,  0.9853, -4.2316, -1.6606])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.reshape">
<span class="sig-name descname"><span class="pre">reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.reshape" title="Link to this definition"></a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
but with the specified shape. This method returns a view if <a class="reference internal" href="#unit_scaling.parameter.Tensor.shape" title="unit_scaling.parameter.Tensor.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a> is
compatible with the current shape. See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is
possible to return a view.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reshape()</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><em>ints</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – the desired shape</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.reshape_as">
<span class="sig-name descname"><span class="pre">reshape_as</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.reshape_as" title="Link to this definition"></a></dt>
<dd><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.reshape_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.reshape(other.sizes())</span></code>.
This method returns a view if <code class="docutils literal notranslate"><span class="pre">other.sizes()</span></code> is compatible with the current
shape. See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is possible to return a view.</p>
<p>Please see <a class="reference internal" href="#unit_scaling.parameter.Tensor.reshape" title="unit_scaling.parameter.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same shape
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.resize_">
<span class="sig-name descname"><span class="pre">resize_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">sizes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.resize_" title="Link to this definition"></a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is a low-level method. The storage is reinterpreted as C-contiguous,
ignoring the current strides (unless the target size equals the current
size, in which case the tensor is left unchanged). For most purposes, you
will instead want to use <a class="reference internal" href="#unit_scaling.parameter.Tensor.view" title="unit_scaling.parameter.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a>, which checks for
contiguity, or <a class="reference internal" href="#unit_scaling.parameter.Tensor.reshape" title="unit_scaling.parameter.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which copies data if needed. To
change the size in-place with custom strides, see <a class="reference internal" href="#unit_scaling.parameter.Tensor.set_" title="unit_scaling.parameter.Tensor.set_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_()</span></code></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.use_deterministic_algorithms()</span></code></a> and
<a class="reference external" href="https://pytorch.org/docs/stable/deterministic.html#torch.utils.deterministic.fill_uninitialized_memory" title="(in PyTorch v2.5)"><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.utils.deterministic.fill_uninitialized_memory</span></code></a> are both set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>, new elements are initialized to prevent nondeterministic behavior
from using the result as an input to an operation. Floating point and
complex values are set to NaN, and integer values are set to the maximum
value.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – the desired size</p></li>
<li><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>. Note that memory format of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is going to be unaffected if <code class="docutils literal notranslate"><span class="pre">self.size()</span></code> matches <code class="docutils literal notranslate"><span class="pre">sizes</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2],</span>
<span class="go">        [ 3,  4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.resize_as_">
<span class="sig-name descname"><span class="pre">resize_as_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.contiguous_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.resize_as_" title="Link to this definition"></a></dt>
<dd><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>. This is equivalent to <code class="docutils literal notranslate"><span class="pre">self.resize_(tensor.size())</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>. Note that memory format of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is going to be unaffected if <code class="docutils literal notranslate"><span class="pre">self.size()</span></code> matches <code class="docutils literal notranslate"><span class="pre">tensor.size()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.resolve_conj">
<span class="sig-name descname"><span class="pre">resolve_conj</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.resolve_conj" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_conj()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.resolve_neg">
<span class="sig-name descname"><span class="pre">resolve_neg</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.resolve_neg" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.resolve_neg()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.retain_grad">
<span class="sig-name descname"><span class="pre">retain_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.retain_grad" title="Link to this definition"></a></dt>
<dd><p>Enables this Tensor to have their <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during
<a class="reference internal" href="#unit_scaling.parameter.Tensor.backward" title="unit_scaling.parameter.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>. This is a no-op for leaf tensors.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.retains_grad">
<span class="sig-name descname"><span class="pre">retains_grad</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.retains_grad" title="Link to this definition"></a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if this Tensor is non-leaf and its <a class="reference internal" href="#unit_scaling.parameter.Tensor.grad" title="unit_scaling.parameter.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> is enabled to be
populated during <a class="reference internal" href="#unit_scaling.parameter.Tensor.backward" title="unit_scaling.parameter.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.roll">
<span class="sig-name descname"><span class="pre">roll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shifts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.roll" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rot90">
<span class="sig-name descname"><span class="pre">rot90</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.rot90" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rot90()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.round">
<span class="sig-name descname"><span class="pre">round</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decimals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.round" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.round_">
<span class="sig-name descname"><span class="pre">round_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decimals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.round_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.round" title="unit_scaling.parameter.Tensor.round"><code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rsqrt">
<span class="sig-name descname"><span class="pre">rsqrt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.rsqrt" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.rsqrt_">
<span class="sig-name descname"><span class="pre">rsqrt_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.rsqrt_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.rsqrt" title="unit_scaling.parameter.Tensor.rsqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter">
<span class="sig-name descname"><span class="pre">scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter_">
<span class="sig-name descname"><span class="pre">scatter_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter_" title="Link to this definition"></a></dt>
<dd><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, its output
index is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by
the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>This is the reverse operation of the manner described in <a class="reference internal" href="#unit_scaling.parameter.Tensor.gather" title="unit_scaling.parameter.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> (if it is a Tensor) should all have
the same number of dimensions. It is also required that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.
Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<p>Moreover, as for <a class="reference internal" href="#unit_scaling.parameter.Tensor.gather" title="unit_scaling.parameter.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When indices are not unique, the behavior is non-deterministic (one of the
values from <code class="docutils literal notranslate"><span class="pre">src</span></code> will be picked arbitrarily) and the gradient will be
incorrect (it will be propagated to all locations in the source that
correspond to the same index)!</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<p>Additionally accepts an optional <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument that allows
specification of an optional reduction operation, which is applied to all
values in the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, the reduction
operation is applied to an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by
its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding
value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>Given a 3-D tensor and reduction using the multiplication operation, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">*=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>Reducing with the addition operation is the same as using
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_add_()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The reduce argument with Tensor <code class="docutils literal notranslate"><span class="pre">src</span></code> is deprecated and will be removed in
a future PyTorch release. Please use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_reduce_()</span></code></a>
instead for more reduction options.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter, can be either empty
or of the same dimensionality as <code class="docutils literal notranslate"><span class="pre">src</span></code>. When empty, the operation
returns <code class="docutils literal notranslate"><span class="pre">self</span></code> unchanged.</p></li>
<li><p><strong>src</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the source element(s) to scatter.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – reduction operation to apply, can be either
<code class="docutils literal notranslate"><span class="pre">'add'</span></code> or <code class="docutils literal notranslate"><span class="pre">'multiply'</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span>
<span class="go">tensor([[ 1,  2,  3,  4,  5],</span>
<span class="go">        [ 6,  7,  8,  9, 10]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0, 4, 0],</span>
<span class="go">        [0, 2, 0, 0, 0],</span>
<span class="go">        [0, 0, 3, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1, 2, 3, 0, 0],</span>
<span class="go">        [6, 7, 0, 0, 8],</span>
<span class="go">        [0, 0, 0, 0, 0]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span>
<span class="gp">... </span>           <span class="mf">1.23</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;multiply&#39;</span><span class="p">)</span>
<span class="go">tensor([[2.0000, 2.0000, 2.4600, 2.0000],</span>
<span class="go">        [2.0000, 2.0000, 2.0000, 2.4600]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span>
<span class="gp">... </span>           <span class="mf">1.23</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">)</span>
<span class="go">tensor([[2.0000, 2.0000, 3.2300, 2.0000],</span>
<span class="go">        [2.0000, 2.0000, 2.0000, 3.2300]])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">scatter_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor:</span></span></span></dt>
<dd></dd></dl>

<p>Writes the value from <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor.  This operation is equivalent to the previous version,
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor filled entirely with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter, can be either empty
or of the same dimensionality as <code class="docutils literal notranslate"><span class="pre">src</span></code>. When empty, the operation
returns <code class="docutils literal notranslate"><span class="pre">self</span></code> unchanged.</p></li>
<li><p><strong>value</strong> (<em>Scalar</em>) – the value to scatter.</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – reduction operation to apply, can be either
<code class="docutils literal notranslate"><span class="pre">'add'</span></code> or <code class="docutils literal notranslate"><span class="pre">'multiply'</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[2., 0., 0., 0., 0.],</span>
<span class="go">        [0., 2., 0., 0., 0.],</span>
<span class="go">        [0., 0., 0., 0., 0.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter_add">
<span class="sig-name descname"><span class="pre">scatter_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter_add" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter_add_">
<span class="sig-name descname"><span class="pre">scatter_add_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter_add_" title="Link to this definition"></a></dt>
<dd><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code></a>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, it is added to
an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>
for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> should have same number of
dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions
<code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>. Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and add, can be
either empty or of the same dimensionality as <code class="docutils literal notranslate"><span class="pre">src</span></code>. When empty, the
operation returns <code class="docutils literal notranslate"><span class="pre">self</span></code> unchanged.</p></li>
<li><p><strong>src</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the source elements to scatter and add</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[1., 0., 0., 1., 1.],</span>
<span class="go">        [0., 1., 0., 0., 0.],</span>
<span class="go">        [0., 0., 1., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">src</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="go">tensor([[2., 0., 0., 1., 1.],</span>
<span class="go">        [0., 2., 0., 0., 0.],</span>
<span class="go">        [0., 0., 2., 1., 1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter_reduce">
<span class="sig-name descname"><span class="pre">scatter_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter_reduce" title="Link to this definition"></a></dt>
<dd><p>Out-of-place version of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.scatter_reduce_">
<span class="sig-name descname"><span class="pre">scatter_reduce_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_self</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.scatter_reduce_" title="Link to this definition"></a></dt>
<dd><p>Reduces all values from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor to the indices specified in
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor using the applied reduction
defined via the <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> argument (<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>). For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, it is reduced to an
index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=&quot;True&quot;</span></code>, the values in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor are included in the reduction.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> should all have
the same number of dimensions. It is also required that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that
<code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.
Note that <code class="docutils literal notranslate"><span class="pre">index</span></code> and <code class="docutils literal notranslate"><span class="pre">src</span></code> do not broadcast.</p>
<p>For a 3-D tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">reduce=&quot;sum&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">include_self=True</span></code> the
output is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation may behave nondeterministically when given tensors on a CUDA device. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward pass is implemented only for <code class="docutils literal notranslate"><span class="pre">src.shape</span> <span class="pre">==</span> <span class="pre">index.shape</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is in beta and may change in the near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and reduce.</p></li>
<li><p><strong>src</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the source elements to scatter and reduce</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the reduction operation to apply for non-unique indices
(<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;prod&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amax&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amin&quot;</span></code>)</p></li>
<li><p><strong>include_self</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether elements from the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor are
included in the reduction</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
<span class="go">tensor([5., 14., 8., 4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([4., 12., 5., 4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;amax&quot;</span><span class="p">)</span>
<span class="go">tensor([5., 6., 5., 2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span><span class="o">.</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;amax&quot;</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">tensor([3., 6., 5., 2.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.select">
<span class="sig-name descname"><span class="pre">select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.select" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.select.html#torch.select" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.select_scatter">
<span class="sig-name descname"><span class="pre">select_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.select_scatter" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select_scatter()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.set_">
<span class="sig-name descname"><span class="pre">set_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.set_" title="Link to this definition"></a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a tensor,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will share the same storage and have the same size and
strides as <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code>. Changes to elements in one tensor will be reflected
in the other.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – the tensor or storage to use</p></li>
<li><p><strong>storage_offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – the offset in the storage</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a><em>, </em><em>optional</em>) – the desired size. Defaults to the size of the source.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>, </em><em>optional</em>) – the desired stride. Defaults to C-contiguous strides.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sgn">
<span class="sig-name descname"><span class="pre">sgn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sgn" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sgn()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sgn_">
<span class="sig-name descname"><span class="pre">sgn_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sgn_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sgn" title="unit_scaling.parameter.Tensor.sgn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sgn()</span></code></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.shape">
<span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.shape" title="Link to this definition"></a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. Alias for <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>.</p>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.size()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.share_memory_">
<span class="sig-name descname"><span class="pre">share_memory_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.share_memory_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.share_memory_" title="Link to this definition"></a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage.share_memory_" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.UntypedStorage.share_memory_()</span></code></a> for more details.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.short">
<span class="sig-name descname"><span class="pre">short</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.short" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>. See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to" title="unit_scaling.parameter.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sigmoid">
<span class="sig-name descname"><span class="pre">sigmoid</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sigmoid" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sigmoid_">
<span class="sig-name descname"><span class="pre">sigmoid_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sigmoid_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sigmoid" title="unit_scaling.parameter.Tensor.sigmoid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sign">
<span class="sig-name descname"><span class="pre">sign</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sign" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sign_">
<span class="sig-name descname"><span class="pre">sign_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sign_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sign" title="unit_scaling.parameter.Tensor.sign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.signbit">
<span class="sig-name descname"><span class="pre">signbit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.signbit" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.signbit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sin">
<span class="sig-name descname"><span class="pre">sin</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sin" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sin_">
<span class="sig-name descname"><span class="pre">sin_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sin_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sin" title="unit_scaling.parameter.Tensor.sin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sinc">
<span class="sig-name descname"><span class="pre">sinc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sinc" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sinc_">
<span class="sig-name descname"><span class="pre">sinc_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sinc_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sinc" title="unit_scaling.parameter.Tensor.sinc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sinh">
<span class="sig-name descname"><span class="pre">sinh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sinh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sinh_">
<span class="sig-name descname"><span class="pre">sinh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sinh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sinh" title="unit_scaling.parameter.Tensor.sinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.size">
<span class="sig-name descname"><span class="pre">size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Size</span> <span class="pre">or</span> <span class="pre">int</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.size" title="Link to this definition"></a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If <code class="docutils literal notranslate"><span class="pre">dim</span></code> is not specified,
the returned value is a <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a>, a subclass of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a>.
If <code class="docutils literal notranslate"><span class="pre">dim</span></code> is specified, returns an int holding the size of that dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The dimension for which to retrieve the size.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">4</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.slice_scatter">
<span class="sig-name descname"><span class="pre">slice_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.slice_scatter" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slice_scatter()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.slogdet">
<span class="sig-name descname"><span class="pre">slogdet</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.slogdet" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.smm">
<span class="sig-name descname"><span class="pre">smm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.smm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.smm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.softmax">
<span class="sig-name descname"><span class="pre">softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.softmax" title="Link to this definition"></a></dt>
<dd><p>Alias for <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sort">
<span class="sig-name descname"><span class="pre">sort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sort" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sparse_dim">
<span class="sig-name descname"><span class="pre">sparse_dim</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sparse_dim" title="Link to this definition"></a></dt>
<dd><p>Return the number of sparse dimensions in a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">0</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.dense_dim" title="unit_scaling.parameter.Tensor.dense_dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.dense_dim()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-hybrid-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">hybrid tensors</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sparse_mask">
<span class="sig-name descname"><span class="pre">sparse_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sparse_mask" title="Link to this definition"></a></dt>
<dd><p>Returns a new <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse tensor</span></a> with values from a
strided tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> filtered by the indices of the sparse
tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>. The values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> sparse tensor are
ignored. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> tensors must have the same
shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned sparse tensor might contain duplicate values if <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
is not coalesced. It is therefore advisable to pass <code class="docutils literal notranslate"><span class="pre">mask.coalesce()</span></code>
if such behavior is not desired.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned sparse tensor has the same indices as the sparse tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>, even when the corresponding values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> are
zeros.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>mask</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – a sparse tensor whose indices are used as a filter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nse</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nse</span><span class="p">,)),</span>
<span class="gp">... </span>               <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nse</span><span class="p">,))],</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nse</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nse</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 2],</span>
<span class="go">                       [0, 1, 4, 3]]),</span>
<span class="go">       values=tensor([[[ 1.6550,  0.2397],</span>
<span class="go">                       [-0.1611, -0.0779]],</span>

<span class="go">                      [[ 0.2326, -1.0558],</span>
<span class="go">                       [ 1.4711,  1.9678]],</span>

<span class="go">                      [[-0.5138, -0.0411],</span>
<span class="go">                       [ 1.9417,  0.5158]],</span>

<span class="go">                      [[ 0.0793,  0.0036],</span>
<span class="go">                       [-0.2569, -0.1055]]]),</span>
<span class="go">       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sparse_resize_">
<span class="sig-name descname"><span class="pre">sparse_resize_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sparse_resize_" title="Link to this definition"></a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse tensor</span></a> to the desired
size and the number of sparse and dense dimensions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the number of specified elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is zero, then
<a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>, <a class="reference internal" href="#unit_scaling.parameter.Tensor.sparse_dim" title="unit_scaling.parameter.Tensor.sparse_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sparse_dim</span></code></a>, and <a class="reference internal" href="#unit_scaling.parameter.Tensor.dense_dim" title="unit_scaling.parameter.Tensor.dense_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dense_dim</span></code></a> can be any
size and positive integers such that <code class="docutils literal notranslate"><span class="pre">len(size)</span> <span class="pre">==</span> <span class="pre">sparse_dim</span> <span class="pre">+</span>
<span class="pre">dense_dim</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> specifies one or more elements, however, then each
dimension in <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> must not be smaller than the corresponding
dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <a class="reference internal" href="#unit_scaling.parameter.Tensor.sparse_dim" title="unit_scaling.parameter.Tensor.sparse_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">sparse_dim</span></code></a> must equal the number
of sparse dimensions in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, and <a class="reference internal" href="#unit_scaling.parameter.Tensor.dense_dim" title="unit_scaling.parameter.Tensor.dense_dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dense_dim</span></code></a> must
equal the number of dense dimensions in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse tensor.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a>) – the desired size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is non-empty
sparse tensor, the desired size cannot be smaller than the
original size.</p></li>
<li><p><strong>sparse_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the number of sparse dimensions</p></li>
<li><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the number of dense dimensions</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sparse_resize_and_clear_">
<span class="sig-name descname"><span class="pre">sparse_resize_and_clear_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sparse_resize_and_clear_" title="Link to this definition"></a></dt>
<dd><p>Removes all specified elements from a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse tensor</span></a> <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> to the desired
size and the number of sparse and dense dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a>) – the desired size.</p></li>
<li><p><strong>sparse_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the number of sparse dimensions</p></li>
<li><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the number of dense dimensions</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.split"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.split" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sqrt">
<span class="sig-name descname"><span class="pre">sqrt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sqrt" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sqrt_">
<span class="sig-name descname"><span class="pre">sqrt_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sqrt_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sqrt" title="unit_scaling.parameter.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.square">
<span class="sig-name descname"><span class="pre">square</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.square" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.square.html#torch.square" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.square()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.square_">
<span class="sig-name descname"><span class="pre">square_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.square_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.square" title="unit_scaling.parameter.Tensor.square"><code class="xref py py-meth docutils literal notranslate"><span class="pre">square()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.squeeze">
<span class="sig-name descname"><span class="pre">squeeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.squeeze" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.squeeze_">
<span class="sig-name descname"><span class="pre">squeeze_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.squeeze_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.squeeze" title="unit_scaling.parameter.Tensor.squeeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sspaddmm">
<span class="sig-name descname"><span class="pre">sspaddmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mat1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sspaddmm" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sspaddmm()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.std">
<span class="sig-name descname"><span class="pre">std</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.std" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.stft">
<span class="sig-name descname"><span class="pre">stft</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_fft</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">hop_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">win_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'reflect'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onesided</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_complex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.stft"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.stft" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.stft()</span></code></a></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function changed signature at version 0.4.1. Calling with
the previous signature may cause error or return incorrect result.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.storage">
<span class="sig-name descname"><span class="pre">storage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/storage.html#torch.TypedStorage" title="(in PyTorch v2.5)"><span class="pre">torch.TypedStorage</span></a></span></span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.storage" title="Link to this definition"></a></dt>
<dd><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">TypedStorage</span></code> is deprecated. It will be removed in the future, and
<code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code> will be the only storage class. To access the
<code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code> directly, use <a class="reference internal" href="#unit_scaling.parameter.Tensor.untyped_storage" title="unit_scaling.parameter.Tensor.untyped_storage"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Tensor.untyped_storage()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.storage_offset">
<span class="sig-name descname"><span class="pre">storage_offset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.storage_offset" title="Link to this definition"></a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of
number of storage elements (not bytes).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.storage_type">
<span class="sig-name descname"><span class="pre">storage_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a></span></span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.storage_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.storage_type" title="Link to this definition"></a></dt>
<dd><p>Returns the type of the underlying storage.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.stride">
<span class="sig-name descname"><span class="pre">stride</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span> <span class="pre">or</span> <span class="pre">int</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.stride" title="Link to this definition"></a></dt>
<dd><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<p>Stride is the jump necessary to go from one element to the next one in the
specified dimension <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension <a class="reference internal" href="#unit_scaling.parameter.Tensor.dim" title="unit_scaling.parameter.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – the desired dimension in which stride is required</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sub">
<span class="sig-name descname"><span class="pre">sub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sub" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sub_">
<span class="sig-name descname"><span class="pre">sub_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sub_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.sub" title="unit_scaling.parameter.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.subtract">
<span class="sig-name descname"><span class="pre">subtract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.subtract" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.subtract()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.subtract_">
<span class="sig-name descname"><span class="pre">subtract_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.subtract_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.subtract" title="unit_scaling.parameter.Tensor.subtract"><code class="xref py py-meth docutils literal notranslate"><span class="pre">subtract()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sum">
<span class="sig-name descname"><span class="pre">sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sum" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.sum_to_size">
<span class="sig-name descname"><span class="pre">sum_to_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.sum_to_size" title="Link to this definition"></a></dt>
<dd><p>Sum <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor to <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a>.
<a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> must be broadcastable to <code class="docutils literal notranslate"><span class="pre">this</span></code> tensor size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – a sequence of integers defining the shape of the output tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.svd">
<span class="sig-name descname"><span class="pre">svd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">some</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_uv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.svd" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.swapaxes">
<span class="sig-name descname"><span class="pre">swapaxes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.swapaxes" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapaxes()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.swapaxes_">
<span class="sig-name descname"><span class="pre">swapaxes_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.swapaxes_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.swapaxes" title="unit_scaling.parameter.Tensor.swapaxes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">swapaxes()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.swapdims">
<span class="sig-name descname"><span class="pre">swapdims</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.swapdims" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.swapdims()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.swapdims_">
<span class="sig-name descname"><span class="pre">swapdims_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.swapdims_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.swapdims" title="unit_scaling.parameter.Tensor.swapdims"><code class="xref py py-meth docutils literal notranslate"><span class="pre">swapdims()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.t">
<span class="sig-name descname"><span class="pre">t</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.t" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.t.html#torch.t" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.t_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.t" title="unit_scaling.parameter.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.take">
<span class="sig-name descname"><span class="pre">take</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.take" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.take_along_dim">
<span class="sig-name descname"><span class="pre">take_along_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.take_along_dim" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take_along_dim()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tan">
<span class="sig-name descname"><span class="pre">tan</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tan" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tan_">
<span class="sig-name descname"><span class="pre">tan_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tan_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.tan" title="unit_scaling.parameter.Tensor.tan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tanh">
<span class="sig-name descname"><span class="pre">tanh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tanh" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tanh_">
<span class="sig-name descname"><span class="pre">tanh_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tanh_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.tanh" title="unit_scaling.parameter.Tensor.tanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tensor_split">
<span class="sig-name descname"><span class="pre">tensor_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices_or_sections</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tensor_split" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor_split()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tile">
<span class="sig-name descname"><span class="pre">tile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tile" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tile()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to" title="Link to this definition"></a></dt>
<dd><p>Performs Tensor dtype and/or device conversion. A <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> are
inferred from the arguments of <code class="docutils literal notranslate"><span class="pre">self.to(*args,</span> <span class="pre">**kwargs)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">self</span></code> Tensor already
has the correct <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, then <code class="docutils literal notranslate"><span class="pre">self</span></code> is returned.
Otherwise, the returned tensor is a copy of <code class="docutils literal notranslate"><span class="pre">self</span></code> with the desired
<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>.</p>
</div>
<p>Here are the ways to call <code class="docutils literal notranslate"><span class="pre">to</span></code>:</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></p>
<dl class="simple">
<dt>Args:</dt><dd><p>memory_format (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional): the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with the specified <a class="reference internal" href="#unit_scaling.parameter.Tensor.device" title="unit_scaling.parameter.Tensor.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> and (optional)
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> it is inferred to be <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert asynchronously with respect to
the host if possible, e.g., converting a CPU Tensor with pinned memory to a
CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>memory_format (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional): the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span></dt>
<dd><blockquote>
<div><p>Returns a Tensor with same <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as
the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert
asynchronously with respect to the host if possible, e.g., converting a CPU
Tensor with pinned memory to a CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</div></blockquote>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Initially dtype=float32, device=cpu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">other</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_dense">
<span class="sig-name descname"><span class="pre">to_dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masked_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_dense" title="Link to this definition"></a></dt>
<dd><p>Creates a strided copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a strided tensor, otherwise returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>{dtype}</strong></p></li>
<li><p><strong>masked_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code> (default) and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> has a sparse layout then the backward of
<a class="reference internal" href="#unit_scaling.parameter.Tensor.to_dense" title="unit_scaling.parameter.Tensor.to_dense"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to_dense()</span></code></a> returns <code class="docutils literal notranslate"><span class="pre">grad.sparse_mask(self)</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span>
<span class="gp">... </span>       <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span>
<span class="gp">... </span>       <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span>
<span class="gp">... </span>       <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_mkldnn">
<span class="sig-name descname"><span class="pre">to_mkldnn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_mkldnn" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of the tensor in <code class="docutils literal notranslate"><span class="pre">torch.mkldnn</span></code> layout.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_padded_tensor">
<span class="sig-name descname"><span class="pre">to_padded_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_padded_tensor" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference internal" href="#unit_scaling.parameter.Tensor.to_padded_tensor" title="unit_scaling.parameter.Tensor.to_padded_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_padded_tensor()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse">
<span class="sig-name descname"><span class="pre">to_sparse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparseDims</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse" title="Link to this definition"></a></dt>
<dd><p>Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
<a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">coordinate format</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sparseDims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – the number of sparse dimensions to include in the new sparse tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1, 1],</span>
<span class="go">                       [0, 2]]),</span>
<span class="go">       values=tensor([ 9, 10]),</span>
<span class="go">       size=(3, 3), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([[ 9,  0, 10]]),</span>
<span class="go">       size=(3, 3), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to_sparse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">blocksize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span></dt>
<dd></dd></dl>

<p>Returns a sparse tensor with the specified layout and blocksize.  If
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, the number of dense dimensions could be
specified, and a hybrid sparse tensor will be created, with
<cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 - dense_dim</cite> batch
dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> layout and blocksize parameters match
with the specified layout and blocksize, return
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. Otherwise, return a sparse tensor copy of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layout</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.layout" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – The desired sparse
layout. One of <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.sparse_csr</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.sparse_csc</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.sparse_bsr</span></code>, or
<code class="docutils literal notranslate"><span class="pre">torch.sparse_bsc</span></code>. Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code>.</p></li>
<li><p><strong>blocksize</strong> (list, tuple, <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a>, optional) – Block size
of the resulting BSR or BSC tensor. For other layouts,
specifying the block size that is not <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in a
RuntimeError exception.  A block size must be a tuple of length
two such that its items evenly divide the two sparse dimensions.</p></li>
<li><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of dense dimensions of the
resulting CSR, CSC, BSR or BSC tensor.  This argument should be
used only if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a
value between 0 and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 2, 2],</span>
<span class="go">                       [0, 0, 1]]),</span>
<span class="go">       values=tensor([1, 2, 3]),</span>
<span class="go">       size=(3, 2), nnz=3, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 1, 2]),</span>
<span class="go">       col_indices=tensor([0, 0]),</span>
<span class="go">       values=tensor([[[1, 0]],</span>
<span class="go">                      [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 1, 3]),</span>
<span class="go">       col_indices=tensor([0, 0, 1]),</span>
<span class="go">       values=tensor([[1],</span>
<span class="go">                      [2],</span>
<span class="go">                      [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse_bsc">
<span class="sig-name descname"><span class="pre">to_sparse_bsc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blocksize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse_bsc" title="Link to this definition"></a></dt>
<dd><p>Convert a tensor to a block sparse column (BSC) storage format of
given blocksize.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, then the number of
dense dimensions could be specified, and a hybrid BSC tensor will be
created, with <cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 -
dense_dim</cite> batch dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blocksize</strong> (list, tuple, <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a>, optional) – Block size
of the resulting BSC tensor. A block size must be a tuple of
length two such that its items evenly divide the two sparse
dimensions.</p></li>
<li><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of dense dimensions of the
resulting BSC tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsc</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">to_sparse_bsc</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsc</span><span class="o">.</span><span class="n">row_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 0, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_bsc</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(ccol_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       row_indices=tensor([0, 1, 0]),</span>
<span class="go">       values=tensor([[[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]]]), size=(4, 3, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_bsc)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse_bsr">
<span class="sig-name descname"><span class="pre">to_sparse_bsr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blocksize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse_bsr" title="Link to this definition"></a></dt>
<dd><p>Convert a tensor to a block sparse row (BSR) storage format of given
blocksize.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is strided, then the number of dense
dimensions could be specified, and a hybrid BSR tensor will be
created, with <cite>dense_dim</cite> dense dimensions and <cite>self.dim() - 2 -
dense_dim</cite> batch dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blocksize</strong> (list, tuple, <a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a>, optional) – Block size
of the resulting BSR tensor. A block size must be a tuple of
length two such that its items evenly divide the two sparse
dimensions.</p></li>
<li><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of dense dimensions of the
resulting BSR tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsr</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse_bsr</span><span class="o">.</span><span class="n">col_indices</span><span class="p">()</span>
<span class="go">tensor([0, 1, 0, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 3]),</span>
<span class="go">       col_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]],</span>


<span class="go">                      [[[1.]],</span>

<span class="go">                       [[1.]]]]), size=(4, 3, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_bsr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse_coo">
<span class="sig-name descname"><span class="pre">to_sparse_coo</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.to_sparse_coo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse_coo" title="Link to this definition"></a></dt>
<dd><p>Convert a tensor to <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">coordinate format</span></a>.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_coo</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse_csc">
<span class="sig-name descname"><span class="pre">to_sparse_csc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse_csc" title="Link to this definition"></a></dt>
<dd><p>Convert a tensor to compressed column storage (CSC) format.  Except
for strided tensors, only works with 2D tensors.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
is strided, then the number of dense dimensions could be specified,
and a hybrid CSC tensor will be created, with <cite>dense_dim</cite> dense
dimensions and <cite>self.dim() - 2 - dense_dim</cite> batch dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of dense dimensions of the
resulting CSC tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csc</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csc</span><span class="p">(</span><span class="n">dense_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor(ccol_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       row_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[1.]],</span>

<span class="go">                      [[1.]],</span>

<span class="go">                      [[1.]]]), size=(3, 3, 1, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_csc)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.to_sparse_csr">
<span class="sig-name descname"><span class="pre">to_sparse_csr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.to_sparse_csr" title="Link to this definition"></a></dt>
<dd><p>Convert a tensor to compressed row storage format (CSR).  Except for
strided tensors, only works with 2D tensors.  If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is
strided, then the number of dense dimensions could be specified, and a
hybrid CSR tensor will be created, with <cite>dense_dim</cite> dense dimensions
and <cite>self.dim() - 2 - dense_dim</cite> batch dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dense_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of dense dimensions of the
resulting CSR tensor.  This argument should be used only if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a strided tensor, and must be a value between 0
and dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor minus two.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sparse</span><span class="o">.</span><span class="n">_nnz</span><span class="p">()</span>
<span class="go">25</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">dense</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">(</span><span class="n">dense_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 2, 3]),</span>
<span class="go">       col_indices=tensor([0, 2, 1]),</span>
<span class="go">       values=tensor([[[1.]],</span>

<span class="go">                      [[1.]],</span>

<span class="go">                      [[1.]]]), size=(3, 3, 1, 1), nnz=3,</span>
<span class="go">       layout=torch.sparse_csr)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tolist">
<span class="sig-name descname"><span class="pre">tolist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span> <span class="pre">or</span> <span class="pre">number</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tolist" title="Link to this definition"></a></dt>
<dd><p>Returns the tensor as a (nested) list. For scalars, a standard
Python number is returned, just like with <a class="reference internal" href="#unit_scaling.parameter.Tensor.item" title="unit_scaling.parameter.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item()</span></code></a>.
Tensors are automatically moved to the CPU first if necessary.</p>
<p>This operation is not differentiable.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[0.012766935862600803, 0.5415473580360413],</span>
<span class="go"> [-0.08909505605697632, 0.7729271650314331]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">0.012766935862600803</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.topk">
<span class="sig-name descname"><span class="pre">topk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">largest</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.topk" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.trace">
<span class="sig-name descname"><span class="pre">trace</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.trace" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.transpose">
<span class="sig-name descname"><span class="pre">transpose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.transpose" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.transpose_">
<span class="sig-name descname"><span class="pre">transpose_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.transpose_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.transpose" title="unit_scaling.parameter.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.triangular_solve">
<span class="sig-name descname"><span class="pre">triangular_solve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">A</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transpose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unitriangular</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#unit_scaling.parameter.Tensor.triangular_solve" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triangular_solve()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tril">
<span class="sig-name descname"><span class="pre">tril</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tril" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.tril_">
<span class="sig-name descname"><span class="pre">tril_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.tril_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.tril" title="unit_scaling.parameter.Tensor.tril"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.triu">
<span class="sig-name descname"><span class="pre">triu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.triu" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.triu_">
<span class="sig-name descname"><span class="pre">triu_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.triu_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.triu" title="unit_scaling.parameter.Tensor.triu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.true_divide">
<span class="sig-name descname"><span class="pre">true_divide</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.true_divide" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.true_divide()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.true_divide_">
<span class="sig-name descname"><span class="pre">true_divide_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.true_divide_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.true_divide_" title="unit_scaling.parameter.Tensor.true_divide_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">true_divide_()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.trunc">
<span class="sig-name descname"><span class="pre">trunc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.trunc" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.trunc_">
<span class="sig-name descname"><span class="pre">trunc_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.trunc_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.trunc" title="unit_scaling.parameter.Tensor.trunc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span> <span class="pre">or</span> <span class="pre">Tensor</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.type" title="Link to this definition"></a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<em>dtype</em><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.type_as">
<span class="sig-name descname"><span class="pre">type_as</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.type_as" title="Link to this definition"></a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to <code class="docutils literal notranslate"><span class="pre">self.type(tensor.type())</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><em>Tensor</em></a>) – the tensor which has the desired type</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unbind">
<span class="sig-name descname"><span class="pre">unbind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">seq</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unbind" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unbind()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unflatten">
<span class="sig-name descname"><span class="pre">unflatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sizes</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.unflatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.unflatten" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unflatten()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unfold">
<span class="sig-name descname"><span class="pre">unfold</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unfold" title="Link to this definition"></a></dt>
<dd><p>Returns a view of the original tensor which contains all slices of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the size of dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> for <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, the size of
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dimension</span></code> in the returned tensor will be
<cite>(sizedim - size) / step + 1</cite>.</p>
<p>An additional dimension of size <a class="reference internal" href="#unit_scaling.parameter.Tensor.size" title="unit_scaling.parameter.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> is appended in the returned tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dimension</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – dimension in which unfolding happens</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the size of each slice that is unfolded</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the step between each slice</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  3.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 4.,  5.],</span>
<span class="go">        [ 5.,  6.],</span>
<span class="go">        [ 6.,  7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.uniform_">
<span class="sig-name descname"><span class="pre">uniform_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">from=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator=None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.uniform_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform
distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{\text{to} - \text{from}}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unique">
<span class="sig-name descname"><span class="pre">unique</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_counts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.unique"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.unique" title="Link to this definition"></a></dt>
<dd><p>Returns the unique elements of the input tensor.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unique_consecutive">
<span class="sig-name descname"><span class="pre">unique_consecutive</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">return_inverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_counts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/_tensor.html#Tensor.unique_consecutive"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#unit_scaling.parameter.Tensor.unique_consecutive" title="Link to this definition"></a></dt>
<dd><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique_consecutive()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unsafe_chunk">
<span class="sig-name descname"><span class="pre">unsafe_chunk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unsafe_chunk" title="Link to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_chunk()</span></code></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unsafe_split">
<span class="sig-name descname"><span class="pre">unsafe_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unsafe_split" title="Link to this definition"></a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsafe_split()</span></code></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unsqueeze">
<span class="sig-name descname"><span class="pre">unsqueeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unsqueeze" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.unsqueeze_">
<span class="sig-name descname"><span class="pre">unsqueeze_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.unsqueeze_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.unsqueeze" title="unit_scaling.parameter.Tensor.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.untyped_storage">
<span class="sig-name descname"><span class="pre">untyped_storage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage" title="(in PyTorch v2.5)"><span class="pre">torch.UntypedStorage</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.untyped_storage" title="Link to this definition"></a></dt>
<dd><p>Returns the underlying <code class="xref py py-class docutils literal notranslate"><span class="pre">UntypedStorage</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.values" title="Link to this definition"></a></dt>
<dd><p>Return the values tensor of a <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" title="(in PyTorch v2.5)"><span class="xref std std-ref">sparse COO tensor</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Throws an error if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is not a sparse COO tensor.</p>
</div>
<p>See also <a class="reference internal" href="#unit_scaling.parameter.Tensor.indices" title="unit_scaling.parameter.Tensor.indices"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.indices()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method can only be called on a coalesced sparse tensor. See
<a class="reference internal" href="#unit_scaling.parameter.Tensor.coalesce" title="unit_scaling.parameter.Tensor.coalesce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.coalesce()</span></code></a> for details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.var">
<span class="sig-name descname"><span class="pre">var</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.var" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.var.html#torch.var" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.vdot">
<span class="sig-name descname"><span class="pre">vdot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.vdot" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vdot()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.view">
<span class="sig-name descname"><span class="pre">view</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.view" title="Link to this definition"></a></dt>
<dd><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <a class="reference internal" href="#unit_scaling.parameter.Tensor.shape" title="unit_scaling.parameter.Tensor.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. For a tensor to be viewed, the new
view size must be compatible with its original size and stride, i.e., each new
view dimension must either be a subspace of an original dimension, or only span
across original dimensions <span class="math notranslate nohighlight">\(d, d+1, \dots, d+k\)</span> that satisfy the following
contiguity-like condition that <span class="math notranslate nohighlight">\(\forall i = d, \dots, d+k-1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]\]</div>
<p>Otherwise, it will not be possible to view <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor as <a class="reference internal" href="#unit_scaling.parameter.Tensor.shape" title="unit_scaling.parameter.Tensor.shape"><code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code></a>
without copying it (e.g., via <a class="reference internal" href="#unit_scaling.parameter.Tensor.contiguous" title="unit_scaling.parameter.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>). When it is unclear whether a
<a class="reference internal" href="#unit_scaling.parameter.Tensor.view" title="unit_scaling.parameter.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> can be performed, it is advisable to use <a class="reference internal" href="#unit_scaling.parameter.Tensor.reshape" title="unit_scaling.parameter.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which
returns a view if the shapes are compatible, and copies (equivalent to calling
<a class="reference internal" href="#unit_scaling.parameter.Tensor.contiguous" title="unit_scaling.parameter.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>) otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.5)"><em>torch.Size</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>...</em>) – the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps 2nd and 3rd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Does not change tensor layout in memory</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">view</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span></dt>
<dd></dd></dl>

<p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>If the element size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is different than that of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>,
then the size of the last dimension of the output will be scaled
proportionally.  For instance, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> element size is twice that of
<code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, then each pair of elements in the last dimension of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> will be combined, and the size of the last dimension of the output
will be half that of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> element size is half that
of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, then each element in the last dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> will
be split in two, and the size of the last dimension of the output will be
double that of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. For this to be possible, the following conditions
must be true:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.dim()</span></code> must be greater than 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.stride(-1)</span></code> must be 1.</p></li>
</ul>
</div></blockquote>
<p>Additionally, if the element size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is greater than that of
<code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>, the following conditions must be true as well:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.size(-1)</span></code> must be divisible by the ratio between the element
sizes of the dtypes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.storage_offset()</span></code> must be divisible by the ratio between the
element sizes of the dtypes.</p></li>
<li><p>The strides of all dimensions, except the last dimension, must be
divisible by the ratio between the element sizes of the dtypes.</p></li>
</ul>
</div></blockquote>
<p>If any of the above conditions are not met, an error is thrown.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This overload is not supported by TorchScript, and using it in a Torchscript
program will cause undefined behavior.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dtype</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>) – the desired dtype</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],</span>
<span class="go">        [-0.1520,  0.7472,  0.5617, -0.8649],</span>
<span class="go">        [-2.4724, -0.0334, -0.2976, -0.8499],</span>
<span class="go">        [-0.2109,  1.9913, -0.9607, -0.6123]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],</span>
<span class="go">        [-1105482831,  1061112040,  1057999968, -1084397505],</span>
<span class="go">        [-1071760287, -1123489973, -1097310419, -1084649136],</span>
<span class="go">        [-1101533110,  1073668768, -1082790149, -1088634448]],</span>
<span class="go">    dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000000000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],</span>
<span class="go">        [-0.1520,  0.7472,  0.5617, -0.8649],</span>
<span class="go">        [-2.4724, -0.0334, -0.2976, -0.8499],</span>
<span class="go">        [-0.2109,  1.9913, -0.9607, -0.6123]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>
<span class="go">tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],</span>
<span class="go">        [-0.1520+0.7472j,  0.5617-0.8649j],</span>
<span class="go">        [-2.4724-0.0334j, -0.2976-0.8499j],</span>
<span class="go">        [-0.2109+1.9913j, -0.9607-0.6123j]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="go">tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,</span>
<span class="go">           8, 191],</span>
<span class="go">        [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,</span>
<span class="go">          93, 191],</span>
<span class="go">        [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,</span>
<span class="go">          89, 191],</span>
<span class="go">        [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,</span>
<span class="go">          28, 191]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 16])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.view_as">
<span class="sig-name descname"><span class="pre">view_as</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.view_as" title="Link to this definition"></a></dt>
<dd><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.view_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.view(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#unit_scaling.parameter.Tensor.view" title="unit_scaling.parameter.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">view</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#unit_scaling.parameter.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.vsplit">
<span class="sig-name descname"><span class="pre">vsplit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_size_or_sections</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span> <span class="pre">of</span> <span class="pre">Tensors</span></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.vsplit" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vsplit()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.where">
<span class="sig-name descname"><span class="pre">where</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.where" title="Link to this definition"></a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code>.
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.where.html#torch.where" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.where()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.xlogy">
<span class="sig-name descname"><span class="pre">xlogy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.xlogy" title="Link to this definition"></a></dt>
<dd><p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.xlogy()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.xlogy_">
<span class="sig-name descname"><span class="pre">xlogy_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.xlogy_" title="Link to this definition"></a></dt>
<dd><p>In-place version of <a class="reference internal" href="#unit_scaling.parameter.Tensor.xlogy" title="unit_scaling.parameter.Tensor.xlogy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">xlogy()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.preserve_format</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.xpu" title="Link to this definition"></a></dt>
<dd><p>Returns a copy of this object in XPU memory.</p>
<p>If this object is already in XPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination XPU device.
Defaults to the current XPU device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>memory_format</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a>, optional) – the desired memory format of
returned Tensor. Default: <code class="docutils literal notranslate"><span class="pre">torch.preserve_format</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="unit_scaling.parameter.Tensor.zero_">
<span class="sig-name descname"><span class="pre">zero_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#unit_scaling.parameter.Tensor" title="unit_scaling.parameter.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#unit_scaling.parameter.Tensor.zero_" title="Link to this definition"></a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="unit_scaling.parameter.Protocol.html" class="btn btn-neutral float-left" title="3.1.24.5. unit_scaling.parameter.Protocol" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="unit_scaling.analysis.html" class="btn btn-neutral float-right" title="3.2. unit_scaling.analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright (c) 2023 Graphcore Ltd. All rights reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>