

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torch._tensor &mdash; unit-scaling  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
    <link rel="shortcut icon" href="../../_static/scales.png"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            unit-scaling
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide.html">1. User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../limitations.html">2. Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference.html">3. API reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">unit-scaling</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">torch._tensor</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for torch._tensor</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">import</span> <span class="nn">copyreg</span>
<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">numbers</span> <span class="kn">import</span> <span class="n">Number</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>
<span class="kn">from</span> <span class="nn">torch._namedtensor_internals</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_serializing_named_tensor</span><span class="p">,</span>
    <span class="n">is_ellipsis</span><span class="p">,</span>
    <span class="n">resolve_ellipsis</span><span class="p">,</span>
    <span class="n">single_ellipsis_index</span><span class="p">,</span>
    <span class="n">unzip_namedshape</span><span class="p">,</span>
    <span class="n">update_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_default_nowrap_functions</span><span class="p">,</span>
    <span class="n">handle_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function_unary</span><span class="p">,</span>
    <span class="n">has_torch_function_variadic</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">assigned</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">WRAPPER_ASSIGNMENTS</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">assigned</span><span class="o">=</span><span class="n">assigned</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/75462</span>
            <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">wrapped</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># Should not be used, this is kept only for BC of loading old serialized Tensor subclasses</span>
<span class="k">def</span> <span class="nf">_rebuild_from_type</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="nb">dict</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">_rebuild_from_type_v2</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">new_type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_type</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="n">new_type</span><span class="p">)</span>
    <span class="c1"># Tensor does define __setstate__ even though it doesn&#39;t define</span>
    <span class="c1"># __getstate__. So only use __setstate__ if it is NOT the one defined</span>
    <span class="c1"># on Tensor</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="s2">&quot;__setstate__&quot;</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span>
    <span class="p">):</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="c1"># NB: If you subclass Tensor, and want to share the subclassed class</span>
<span class="c1"># across processes, you must also update torch/multiprocessing/reductions.py</span>
<span class="c1"># to define a ForkingPickler serialization mode for the class.</span>
<span class="c1">#</span>
<span class="c1"># NB: If you add a new method to Tensor, you must update</span>
<span class="c1"># torch/_C/__init__.pyi.in to add a type annotation for your method;</span>
<span class="c1"># otherwise, it will not show up in autocomplete.</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Only Tensors created explicitly by the user &quot;</span>
                <span class="s2">&quot;(graph leaves) support the deepcopy protocol at the moment.  &quot;</span>
                <span class="s2">&quot;If you were attempting to deepcopy a module, this may be because &quot;</span>
                <span class="s2">&quot;of a torch.nn.utils.weight_norm usage, &quot;</span>
                <span class="s2">&quot;see https://github.com/pytorch/pytorch/pull/103001&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># TODO: skipping storage copy is wrong for meta, as meta</span>
            <span class="c1"># does accurate alias tracking; however, the code below</span>
            <span class="c1"># doesn&#39;t work because of</span>
            <span class="c1"># https://github.com/pytorch/pytorch/issues/47442</span>
            <span class="c1"># Update the test in test_serialization if you remove &#39;meta&#39; from here</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
                <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lazy&quot;</span><span class="p">,</span> <span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;mtia&quot;</span><span class="p">,</span> <span class="s2">&quot;mps&quot;</span><span class="p">,</span> <span class="s2">&quot;maia&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">,</span> <span class="s2">&quot;ipu&quot;</span><span class="p">]</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;The default implementation of __deepcopy__() for wrapper subclasses &quot;</span>
                        <span class="s2">&quot;only works for subclass types that implement clone() and for which &quot;</span>
                        <span class="s2">&quot;cloning returns another instance of the same subclass. You should either &quot;</span>
                        <span class="s2">&quot;properly implement clone() for your subclass or override __deepcopy__() &quot;</span>
                        <span class="s2">&quot;if it is intended behavior for clone() to return an instance of a &quot;</span>
                        <span class="s2">&quot;different type.&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_deepcopy</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                    <span class="c1"># quantizer_params can be different type based on torch attribute</span>
                    <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                    <span class="p">]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Unsupported qscheme </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2"> in deepcopy&quot;</span>
                        <span class="p">)</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
                    <span class="c1"># need to wrap with TypedStorage</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">new_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                            <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">quantizer_params</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for quantized tensors &quot;</span>
                            <span class="s2">&quot;expects the tensor returned by torch._utils._rebuild_qtensor() to &quot;</span>
                            <span class="s2">&quot;match the type of the instance being copied. If you encounter this, &quot;</span>
                            <span class="s2">&quot;please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_empty</span><span class="p">([])</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for non-wrapper subclasses &quot;</span>
                            <span class="s2">&quot;only works for subclass types that implement new_empty() and for which &quot;</span>
                            <span class="s2">&quot;that function returns another instance of the same subclass. You should &quot;</span>
                            <span class="s2">&quot;either properly implement new_empty() for your subclass or override &quot;</span>
                            <span class="s2">&quot;__deepcopy__() if it is intended behavior for new_empty() to return &quot;</span>
                            <span class="s2">&quot;an instance of a different type.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">new_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span>
                        <span class="n">new_storage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">conj_physical</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_neg</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Type of deepcopy result does not match the type of the source tensor. &quot;</span>
                        <span class="s2">&quot;If you encounter this, please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># Plain Tensors don&#39;t have slots</span>
                <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">copyreg</span><span class="o">.</span><span class="n">_slotnames</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">for</span> <span class="n">slot</span> <span class="ow">in</span> <span class="n">slots_to_save</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">):</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">,</span> <span class="n">slot</span><span class="p">,</span> <span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">),</span> <span class="n">memo</span><span class="p">))</span>

            <span class="n">new_tensor</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>

            <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_tensor</span>
            <span class="k">return</span> <span class="n">new_tensor</span>

    <span class="k">def</span> <span class="nf">__reduce_ex__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">materialize_fake_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">materialize_fake_tensors</span>
        <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_get_obj_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Ignore all state when using FakeTensor with skip_data(materialize_fake_tensors) because FakeTensor has</span>
        <span class="c1"># some state that cannot be pickled</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="c1"># TODO: remove hasattr, it&#39;s a hack to support versions of torch that</span>
            <span class="c1"># don&#39;t have _subclasses</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;_subclasses&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span>
            <span class="ow">and</span> <span class="n">materialize_fake_tensors</span>
        <span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">state</span><span class="p">):</span>
            <span class="c1"># Fast path for regular tensor without Python state.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__reduce_ex__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">)</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">_rebuild_from_type_v2</span><span class="p">,</span> <span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">))</span>

<div class="viewcode-block" id="Tensor.storage">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.storage">[docs]</a>
    <span class="k">def</span> <span class="nf">storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        storage() -&gt; torch.TypedStorage</span>

<span class="sd">        Returns the underlying :class:`TypedStorage`.</span>

<span class="sd">        .. warning::</span>

<span class="sd">            :class:`TypedStorage` is deprecated. It will be removed in the future, and</span>
<span class="sd">            :class:`UntypedStorage` will be the only storage class. To access the</span>
<span class="sd">            :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span></div>


    <span class="c1"># For internal use only, to avoid raising deprecation warning</span>
    <span class="k">def</span> <span class="nf">_typed_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reduce_ex_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">check_serializing_named_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">torch.utils.hooks</span> <span class="kn">import</span> <span class="n">warn_if_has_hooks</span>

        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="n">warn_if_has_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">backward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="n">skip_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">skip_data</span>
        <span class="n">materialize_fake_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">serialization</span><span class="o">.</span><span class="n">_serialization_tls</span><span class="o">.</span><span class="n">materialize_fake_tensors</span>
        <span class="p">)</span>

        <span class="c1"># Note: Numpy array is chosen to be the rebuild component for XLA, MTIA, MAIA Tensors.</span>
        <span class="c1"># We considered a few options:</span>
        <span class="c1"># 1. CPU tensor can&#39;t be used here.</span>
        <span class="c1">#    Otherwise in torch.load CPU storage is reconstructed with randomly</span>
        <span class="c1">#    initialized data, moved onto backend device, and then storage is updated</span>
        <span class="c1">#    to the serialized content. This works perfectly for CPU/CUDA but not these backends;</span>
        <span class="c1">#    their tensors are disconnected with storage so they don&#39;t get the update.</span>
        <span class="c1"># 2. Python list is not a good fit due to performance reason.</span>
        <span class="c1">#    `tolist()` converts every single element in the tensor into python objects</span>
        <span class="c1">#    and serialize them one by one.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;mtia&quot;</span><span class="p">,</span> <span class="s2">&quot;maia&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn&#39;t</span>
            <span class="c1"># support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,</span>
            <span class="c1"># this would reconstruct the BFloat16 tensor from numpy.</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize tensors on backends with no storage under skip_data context manager&quot;</span>
                <span class="p">)</span>
            <span class="n">numpy_tensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_device_tensor_from_numpy</span><span class="p">,</span>
                <span class="p">(</span><span class="n">numpy_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;meta&quot;</span><span class="p">:</span>
            <span class="c1"># NB: This implementation BREAKS storage sharing.  Current</span>
            <span class="c1"># hypothesis is that no one cares for meta tensors.</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Serializing tensors on the meta device under skip_data context manager is a no-op&quot;</span>
                <span class="p">)</span>
            <span class="n">arg_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_meta_tensor_no_storage</span><span class="p">,</span> <span class="n">arg_meta</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize qtensor under skip_data context manager, file an issue if you need this feature&quot;</span>
                <span class="p">)</span>
            <span class="c1"># quantizer_params can be different type based on torch attribute</span>
            <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="c1"># convert scales and zero points to tuple to avoid recursive calls</span>
                <span class="c1"># when/if we get multi-axis quantized tensors in the future, the shape</span>
                <span class="c1"># is recoverable from the main tensor shape</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Serialization is not supported for tensors of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
            <span class="c1"># need to wrap with TypedStorage</span>
            <span class="n">args_qtensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="n">quantizer_params</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">,</span> <span class="n">args_qtensor</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="n">args_sparse</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;sparse tensor __reduce_ex__ for layout `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">}:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ccol_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="n">args_sparse_compressed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">compressed_indices</span><span class="p">,</span>
                    <span class="n">plain_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse_compressed</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_nested</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_data</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot serialize nested tensor under skip_data context manager, file an issue if you need this feature&quot;</span>
                <span class="p">)</span>
            <span class="n">args_nested</span> <span class="o">=</span> <span class="p">(</span>
                <span class="c1"># NB: values() currently returns the storage as a buffer in an unsafe way.</span>
                <span class="c1"># Ideally, we&#39;d use a private API for this instead. TODO: Switch to this if</span>
                <span class="c1"># we ever get around to adding it.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_size</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_strides</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nested_tensor_storage_offsets</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_nested_tensor</span><span class="p">,</span> <span class="n">args_nested</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">functional_tensor</span><span class="o">.</span><span class="n">FunctionalTensor</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">arg_wrapper_subclass</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_wrapper_subclass</span><span class="p">,</span> <span class="n">arg_wrapper_subclass</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="n">skip_data</span> <span class="ow">and</span> <span class="n">materialize_fake_tensors</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">arg_wrapper_subclass</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_wrapper_subclass</span><span class="p">,</span> <span class="n">arg_wrapper_subclass</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">v3_dtypes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_new_dtypes</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">v3_dtypes</span><span class="p">:</span>
                <span class="n">rebuild_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v3</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
                <span class="c1"># need to wrap with TypedStorage</span>
                <span class="n">rebuild_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v2</span>  <span class="c1"># type: ignore[assignment]</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="c1"># TODO: remove hasattr, it&#39;s a hack to support versions of torch that</span>
            <span class="c1"># don&#39;t have _subclasses</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;_subclasses&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_subclasses</span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n">FakeTensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">skip_data</span>
            <span class="p">):</span>
                <span class="n">storage</span><span class="o">.</span><span class="n">_fake_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">storage</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1"># previously was self._backward_hooks</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="n">metadata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">get_tensor_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="n">metadata</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="k">return</span> <span class="p">(</span><span class="n">rebuild_func</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Warning: this method is NOT called when you torch.load() a tensor;</span>
        <span class="c1"># that is managed by _rebuild_tensor_v2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;__setstate__ can be only called on leaf Tensors&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Variable</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># The setting of _backward_hooks is expected to be a no-op.</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span>
            <span class="p">)</span>
        <span class="c1"># All strings are unicode in Python 3.</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_tensor_str</span><span class="o">.</span><span class="n">_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.backward">[docs]</a>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the gradient of current tensor wrt graph leaves.</span>

<span class="sd">        The graph is differentiated using the chain rule. If the tensor is</span>
<span class="sd">        non-scalar (i.e. its data has more than one element) and requires</span>
<span class="sd">        gradient, the function additionally requires specifying a ``gradient``.</span>
<span class="sd">        It should be a tensor of matching type and shape, that represents</span>
<span class="sd">        the gradient of the differentiated function w.r.t. ``self``.</span>

<span class="sd">        This function accumulates gradients in the leaves - you might need to zero</span>
<span class="sd">        ``.grad`` attributes or set them to ``None`` before calling it.</span>
<span class="sd">        See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`</span>
<span class="sd">        for details on the memory layout of accumulated gradients.</span>

<span class="sd">        .. note::</span>

<span class="sd">            If you run any forward ops, create ``gradient``, and/or call ``backward``</span>
<span class="sd">            in a user-specified CUDA stream context, see</span>
<span class="sd">            :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.</span>

<span class="sd">        .. note::</span>

<span class="sd">            When ``inputs`` are provided and a given input is not a leaf,</span>
<span class="sd">            the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).</span>
<span class="sd">            It is an implementation detail on which the user should not rely.</span>
<span class="sd">            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.</span>

<span class="sd">        Args:</span>
<span class="sd">            gradient (Tensor, optional): The gradient of the function</span>
<span class="sd">                being differentiated w.r.t. ``self``.</span>
<span class="sd">                This argument can be omitted if ``self`` is a scalar.</span>
<span class="sd">            retain_graph (bool, optional): If ``False``, the graph used to compute</span>
<span class="sd">                the grads will be freed. Note that in nearly all cases setting</span>
<span class="sd">                this option to True is not needed and often can be worked around</span>
<span class="sd">                in a much more efficient way. Defaults to the value of</span>
<span class="sd">                ``create_graph``.</span>
<span class="sd">            create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">                be constructed, allowing to compute higher order derivative</span>
<span class="sd">                products. Defaults to ``False``.</span>
<span class="sd">            inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be</span>
<span class="sd">                accumulated into ``.grad``. All other tensors will be ignored. If not</span>
<span class="sd">                provided, the gradient is accumulated into all the leaf Tensors that were</span>
<span class="sd">                used to compute the :attr:`tensors`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.register_hook">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.register_hook">[docs]</a>
    <span class="k">def</span> <span class="nf">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook.</span>

<span class="sd">        The hook will be called every time a gradient with respect to the</span>
<span class="sd">        Tensor is computed. The hook should have the following signature::</span>

<span class="sd">            hook(grad) -&gt; Tensor or None</span>


<span class="sd">        The hook should not modify its argument, but it can optionally return</span>
<span class="sd">        a new gradient which will be used in place of :attr:`grad`.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        .. note::</span>
<span class="sd">            See :ref:`backward-hooks-execution` for more information on how when this hook</span>
<span class="sd">            is executed, and how its execution is ordered relative to other hooks.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v.grad</span>

<span class="sd">             2</span>
<span class="sd">             4</span>
<span class="sd">             6</span>
<span class="sd">            [torch.FloatTensor of size (3,)]</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;cannot register a hook on a tensor that doesn&#39;t require gradient&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_register_hook_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">torch.utils.hooks</span> <span class="kn">import</span> <span class="n">RemovableHandle</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


<div class="viewcode-block" id="Tensor.register_post_accumulate_grad_hook">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.register_post_accumulate_grad_hook">[docs]</a>
    <span class="k">def</span> <span class="nf">register_post_accumulate_grad_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook that runs after grad accumulation.</span>

<span class="sd">        The hook will be called after all gradients for a tensor have been accumulated,</span>
<span class="sd">        meaning that the .grad field has been updated on that tensor. The post</span>
<span class="sd">        accumulate grad hook is ONLY applicable for leaf tensors (tensors without a</span>
<span class="sd">        .grad_fn field). Registering this hook on a non-leaf tensor will error!</span>

<span class="sd">        The hook should have the following signature::</span>

<span class="sd">            hook(param: Tensor) -&gt; None</span>

<span class="sd">        Note that, unlike other autograd hooks, this hook operates on the tensor</span>
<span class="sd">        that requires grad and not the grad itself. The hook can in-place modify</span>
<span class="sd">        and access its Tensor argument, including its .grad field.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        .. note::</span>
<span class="sd">            See :ref:`backward-hooks-execution` for more information on how when this hook</span>
<span class="sd">            is executed, and how its execution is ordered relative to other hooks. Since</span>
<span class="sd">            this hook runs during the backward pass, it will run in no_grad mode (unless</span>
<span class="sd">            create_graph is True). You can use torch.enable_grad() to re-enable autograd</span>
<span class="sd">            within the hook if you need it.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">            &gt;&gt;&gt; # simulate a simple SGD update</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v</span>
<span class="sd">            tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">register_post_accumulate_grad_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;cannot register a hook on a tensor that doesn&#39;t require gradient&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;post accumulate grad hooks cannot be registered on non-leaf tensors&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

        <span class="kn">from</span> <span class="nn">torch.utils.hooks</span> <span class="kn">import</span> <span class="n">RemovableHandle</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>


    <span class="k">def</span> <span class="nf">reinforce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)])</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="n">trim</span><span class="p">(</span>
<span class="w">                </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;reinforce() was removed.</span>
<span class="sd">            Use torch.distributions instead.</span>
<span class="sd">            See https://pytorch.org/docs/main/distributions.html</span>

<span class="sd">            Instead of:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            action = probs.multinomial()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            action.reinforce(reward)</span>
<span class="sd">            action.backward()</span>

<span class="sd">            Use:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            # NOTE: categorical is equivalent to what used to be called multinomial</span>
<span class="sd">            m = torch.distributions.Categorical(probs)</span>
<span class="sd">            action = m.sample()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            loss = -m.log_prob(action) * reward</span>
<span class="sd">            loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">detach</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor, detached from the current graph.</span>

<span class="sd">    The result will never require gradient.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>

<span class="sd">    .. note::</span>

<span class="sd">      Returned Tensor shares the same storage with the original one.</span>
<span class="sd">      In-place modifications on either of them will be seen, and may trigger</span>
<span class="sd">      errors in correctness checks.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">detach_</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">detach_</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Detaches the Tensor from the graph that created it, making it a leaf.</span>
<span class="sd">    Views cannot be detached in-place.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="Tensor.is_shared">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.is_shared">[docs]</a>
    <span class="k">def</span> <span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checks if tensor is in shared memory.</span>

<span class="sd">        This is always ``True`` for CUDA tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">is_shared</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_is_shared</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.share_memory_">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.share_memory_">[docs]</a>
    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the underlying storage to shared memory.</span>

<span class="sd">        This is a no-op if the underlying storage is already in shared memory</span>
<span class="sd">        and for CUDA tensors. Tensors in shared memory cannot be resized.</span>

<span class="sd">        See :meth:`torch.UntypedStorage.share_memory_` for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_share_memory_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="Tensor.module_load">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.module_load">[docs]</a>
    <span class="k">def</span> <span class="nf">module_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">assign</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines how to transform ``other`` when loading it into ``self`` in :meth:`~nn.Module.load_state_dict`.</span>

<span class="sd">        Used when :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.</span>

<span class="sd">        It is expected that ``self`` is a parameter or buffer in an ``nn.Module`` and ``other`` is the</span>
<span class="sd">        value in the state dictionary with the corresponding key, this method defines</span>
<span class="sd">        how ``other`` is remapped before being swapped with ``self`` via</span>
<span class="sd">        :func:`~torch.utils.swap_tensors` in :meth:`~nn.Module.load_state_dict`.</span>

<span class="sd">        .. note::</span>
<span class="sd">            This method should always return a new object that is not ``self`` or ``other``.</span>
<span class="sd">            For example, the default implementation returns ``self.copy_(other).detach()``</span>
<span class="sd">            if ``assign`` is ``False`` or ``other.detach()`` if ``assign`` is ``True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            other (Tensor): value in state dict with key corresponding to ``self``</span>
<span class="sd">            assign (bool): the assign argument passed to :meth:`nn.Module.load_state_dict`</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">module_load</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">assign</span><span class="o">=</span><span class="n">assign</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">assign</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">other</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span></div>


    <span class="k">def</span> <span class="fm">__reversed__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reverses the tensor along dimension 0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__reversed__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.norm">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.norm">[docs]</a>
    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;fro&quot;</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.norm`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._linalg_utils</span> <span class="kn">import</span> <span class="n">solve</span>

        <span class="k">return</span> <span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._linalg_utils</span> <span class="kn">import</span> <span class="n">lstsq</span>

        <span class="k">return</span> <span class="n">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._linalg_utils</span> <span class="kn">import</span> <span class="n">eig</span>

        <span class="k">return</span> <span class="n">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._linalg_utils</span> <span class="kn">import</span> <span class="n">_symeig</span>

        <span class="k">return</span> <span class="n">_symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.lu">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.lu">[docs]</a>
    <span class="k">def</span> <span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.lu`&quot;&quot;&quot;</span>
        <span class="c1"># If get_infos is True, then we don&#39;t need to check for errors and vice versa</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">lu</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="n">get_infos</span>
            <span class="p">)</span>

        <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_lu_with_info</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">check_errors</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="n">get_infos</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">get_infos</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span></div>


<div class="viewcode-block" id="Tensor.stft">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.stft">[docs]</a>
    <span class="k">def</span> <span class="nf">stft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.stft`</span>

<span class="sd">        .. warning::</span>
<span class="sd">          This function changed signature at version 0.4.1. Calling with</span>
<span class="sd">          the previous signature may cause error or return incorrect result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">stft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.istft">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.istft">[docs]</a>
    <span class="k">def</span> <span class="nf">istft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.istft`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">istft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">istft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">length</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resize_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize_as</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize_as is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<div class="viewcode-block" id="Tensor.split">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.split">[docs]</a>
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.split`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">split_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">split_size</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unique">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.unique">[docs]</a>
    <span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique elements of the input tensor.</span>

<span class="sd">        See :func:`torch.unique`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
            <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
            <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unique_consecutive">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.unique_consecutive">[docs]</a>
    <span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Eliminates all but the first element from every consecutive group of equivalent elements.</span>

<span class="sd">        See :func:`torch.unique_consecutive`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
        <span class="p">)</span></div>


    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="nf">__rdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">other</span>

    <span class="fm">__rtruediv__</span> <span class="o">=</span> <span class="n">__rdiv__</span>
    <span class="fm">__itruediv__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">__idiv__</span>

    <span class="fm">__pow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">pow</span>
    <span class="p">)</span>
    <span class="fm">__ipow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">pow_</span>
    <span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__format__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_meta</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rlshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_left_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rrshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="fm">__pos__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">positive</span>
    <span class="fm">__neg__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">neg</span>
    <span class="fm">__abs__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">abs</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__len__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Using len to get tensor shape might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Recommended usage would be tensor.shape[0]. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results.&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: we use &#39;imap&#39; and not &#39;map&#39; here, so that in Python 2 we get a</span>
        <span class="c1"># generator and don&#39;t eagerly perform all the indexes.  This could</span>
        <span class="c1"># save us work, and also helps keep trace ordering deterministic</span>
        <span class="c1"># (e.g., if you zip(*hiddens), the eager map will force all the</span>
        <span class="c1"># indexes of hiddens[0] before hiddens[1], while the generator</span>
        <span class="c1"># map will interleave them.)</span>
        <span class="c1"># NB: We have intentionally skipped __torch_function__ dispatch here.</span>
        <span class="c1"># See gh-54457</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Iterating over a tensor might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape won&#39;t change the number of &quot;</span>
                <span class="s2">&quot;iterations executed (and might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results).&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Do NOT handle __torch_function__ here as user&#39;s default</span>
        <span class="c1"># implementation that handle most functions will most likely do it wrong.</span>
        <span class="c1"># It can be easily overridden by defining this method on the user</span>
        <span class="c1"># subclass if needed.</span>
        <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">tensor_methods</span> <span class="o">=</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="n">tensor_methods</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;volatile&quot;</span><span class="p">)</span>  <span class="c1"># deprecated</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">tensor_methods</span> <span class="o">+</span> <span class="n">attrs</span>

        <span class="c1"># property only available dense, cuda tensors</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;__cuda_array_interface__&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

    <span class="c1"># Numpy array interface, to support `numpy.asarray(tensor) -&gt; ndarray`</span>
    <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># prefer Tensor ops over numpy ones</span>

    <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__array__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Wrap Numpy array again in a suitable tensor when done, to support e.g.</span>
    <span class="c1"># `numpy.sin(tensor) -&gt; tensor` or `numpy.greater(tensor, 0) -&gt; ByteTensor`</span>
    <span class="k">def</span> <span class="nf">__array_wrap__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">__array_wrap__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="n">array</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1"># Workaround, torch has no built-in bool tensor</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check if `element` is present in tensor</span>

<span class="sd">        Args:</span>
<span class="sd">            element (Tensor or scalar): element to be checked</span>
<span class="sd">                for presence in current tensor&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">element</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Number</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymBool</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="c1"># type hint doesn&#39;t understand the __contains__ result array</span>
            <span class="k">return</span> <span class="nb">bool</span><span class="p">((</span><span class="n">element</span> <span class="o">==</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[union-attr]</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Tensor.__contains__ only supports Tensor or scalar, but you passed in a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">element</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">__cuda_array_interface__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Array view description for cuda tensors.</span>

<span class="sd">        See:</span>
<span class="sd">        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">__cuda_array_interface__</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># raise AttributeError for unsupported tensors, so that</span>
        <span class="c1"># hasattr(cpu_tensor, &quot;__cuda_array_interface__&quot;) is False.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on non-CUDA tensor type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;If CUDA data is required use tensor.cuda() to copy tensor to device memory.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on sparse type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Use Tensor.to_dense() to convert to a dense tensor first.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># RuntimeError, matching tensor.__array__() behavior.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on Variable that requires grad. &quot;</span>
                <span class="s2">&quot;If gradients aren&#39;t required, use var.detach() to get Variable that doesn&#39;t require grad.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># CUDA devices are little-endian and tensors are stored in native byte</span>
        <span class="c1"># order. 1-byte entries are endian-agnostic.</span>
        <span class="n">typestr</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="s2">&quot;&lt;c8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="s2">&quot;&lt;c16&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="s2">&quot;&lt;f4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="s2">&quot;&lt;f8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">&quot;|u1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">&quot;|i1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint16</span><span class="p">:</span> <span class="s2">&quot;&lt;u2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">&quot;&lt;i2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint32</span><span class="p">:</span> <span class="s2">&quot;&lt;u4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="s2">&quot;&lt;i4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint64</span><span class="p">:</span> <span class="s2">&quot;&lt;u8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="s2">&quot;&lt;i8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span> <span class="s2">&quot;|b1&quot;</span><span class="p">,</span>
        <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

        <span class="n">itemsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="c1"># __cuda_array_interface__ v2 requires the strides to be omitted</span>
            <span class="c1"># (either not set or set to None) for C-contiguous arrays.</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">itemsize</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
        <span class="n">data_ptr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_ptr</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># read-only is false</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">typestr</span><span class="o">=</span><span class="n">typestr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.storage_type">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.storage_type">[docs]</a>
    <span class="k">def</span> <span class="nf">storage_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;storage_type() -&gt; type</span>

<span class="sd">        Returns the type of the underlying storage.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage_type</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_get_legacy_storage_class</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.refine_names">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.refine_names">[docs]</a>
    <span class="k">def</span> <span class="nf">refine_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Refines the dimension names of :attr:`self` according to :attr:`names`.</span>

<span class="sd">        Refining is a special case of renaming that &quot;lifts&quot; unnamed dimensions.</span>
<span class="sd">        A ``None`` dim can be refined to have any name; a named dim can only be</span>
<span class="sd">        refined to have the same name.</span>

<span class="sd">        Because named tensors can coexist with unnamed tensors, refining names</span>
<span class="sd">        gives a nice way to write named-tensor-aware code that works with both</span>
<span class="sd">        named and unnamed tensors.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded greedily; it is expanded in-place to fill</span>
<span class="sd">        :attr:`names` to the same length as ``self.dim()`` using names from the</span>
<span class="sd">        corresponding indices of ``self.names``.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired names of the output tensor. May</span>
<span class="sd">                contain up to one Ellipsis.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs = imgs.refine_names(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs.names</span>
<span class="sd">            (&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11)</span>
<span class="sd">            &gt;&gt;&gt; tensor = tensor.refine_names(&#39;A&#39;, ..., &#39;B&#39;, &#39;C&#39;)</span>
<span class="sd">            &gt;&gt;&gt; tensor.names</span>
<span class="sd">            (&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="n">resolve_ellipsis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;refine_names&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="n">names</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.align_to">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.align_to">[docs]</a>
    <span class="k">def</span> <span class="nf">align_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Permutes the dimensions of the :attr:`self` tensor to match the order</span>
<span class="sd">        specified in :attr:`names`, adding size-one dims for any new names.</span>

<span class="sd">        All of the dims of :attr:`self` must be named in order to use this method.</span>
<span class="sd">        The resulting tensor is a view on the original tensor.</span>

<span class="sd">        All dimension names of :attr:`self` must be present in :attr:`names`.</span>
<span class="sd">        :attr:`names` may contain additional names that are not in ``self.names``;</span>
<span class="sd">        the output tensor has a size-one dimension for each of those new names.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`</span>
<span class="sd">        that are not mentioned in :attr:`names`, in the order that they appear</span>
<span class="sd">        in :attr:`self`.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired dimension ordering of the</span>
<span class="sd">                output tensor. May contain up to one Ellipsis that is expanded</span>
<span class="sd">                to all unmentioned dim names of :attr:`self`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2)</span>
<span class="sd">            &gt;&gt;&gt; named_tensor = tensor.refine_names(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;)</span>

<span class="sd">            # Move the F and E dims to the front while keeping the rest in order</span>
<span class="sd">            &gt;&gt;&gt; named_tensor.align_to(&#39;F&#39;, &#39;E&#39;, ...)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">ellipsis_idx</span> <span class="o">=</span> <span class="n">single_ellipsis_index</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;align_to&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ellipsis_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span>
            <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_ellipsis</span><span class="p">(</span><span class="n">name</span><span class="p">)],</span> <span class="n">ellipsis_idx</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.unflatten">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.unflatten">[docs]</a>
    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        unflatten(dim, sizes) -&gt; Tensor</span>

<span class="sd">        See :func:`torch.unflatten`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">unflatten</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unflatten: sizes must be non-empty&quot;</span><span class="p">)</span>

        <span class="n">names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip_namedshape</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.rename_">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.rename_">[docs]</a>
    <span class="k">def</span> <span class="nf">rename_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;In-place version of :meth:`~Tensor.rename`.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># Note [rename_ / rename API]</span>
        <span class="c1"># The Python API for these is different from the C++ API. In Python:</span>
        <span class="c1"># 1) tensor.rename(*names) takes a vararglist of names</span>
        <span class="c1"># 2) tensor.rename(**rename_map) takes a map of names to rename.</span>
        <span class="c1"># C++ is static, making it difficult to implement similar behavior.</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.rename">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.rename">[docs]</a>
    <span class="k">def</span> <span class="nf">rename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Renames dimension names of :attr:`self`.</span>

<span class="sd">        There are two main usages:</span>

<span class="sd">        ``self.rename(**rename_map)`` returns a view on tensor that has dims</span>
<span class="sd">        renamed as specified in the mapping :attr:`rename_map`.</span>

<span class="sd">        ``self.rename(*names)`` returns a view on tensor, renaming all</span>
<span class="sd">        dimensions positionally using :attr:`names`.</span>
<span class="sd">        Use ``self.rename(None)`` to drop names on a tensor.</span>

<span class="sd">        One cannot specify both positional args :attr:`names` and keyword args</span>
<span class="sd">        :attr:`rename_map`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;))</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(N=&#39;batch&#39;, C=&#39;channels&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(None)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (None, None, None, None)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tensor.to_sparse_coo">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.to_sparse_coo">[docs]</a>
    <span class="k">def</span> <span class="nf">to_sparse_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert a tensor to :ref:`coordinate format &lt;sparse-coo-docs&gt;`.</span>

<span class="sd">        Examples::</span>

<span class="sd">             &gt;&gt;&gt; dense = torch.randn(5, 5)</span>
<span class="sd">             &gt;&gt;&gt; sparse = dense.to_sparse_coo()</span>
<span class="sd">             &gt;&gt;&gt; sparse._nnz()</span>
<span class="sd">             25</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span></div>


<div class="viewcode-block" id="Tensor.dim_order">
<a class="viewcode-back" href="../../generated/unit_scaling.parameter.Tensor.html#unit_scaling.parameter.Tensor.dim_order">[docs]</a>
    <span class="k">def</span> <span class="nf">dim_order</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        dim_order() -&gt; tuple</span>

<span class="sd">        Returns a tuple of int describing the dim order or physical layout of :attr:`self`.</span>

<span class="sd">        Args:</span>
<span class="sd">            None</span>

<span class="sd">        Dim order represents how dimensions are laid out in memory,</span>
<span class="sd">        starting from the outermost to the innermost dimension.</span>

<span class="sd">        Example::</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).dim_order()</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()</span>
<span class="sd">            (0, 2, 3, 1)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The dim_order tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">dim_order</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="kn">import</span> <span class="nn">torch._prims_common</span> <span class="k">as</span> <span class="nn">utils</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">compute_elementwise_output_logical_to_physical_perm</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span></div>


    <span class="k">def</span> <span class="nf">_update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">_update_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename_</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This __torch_function__ implementation wraps subclasses such that</span>
<span class="sd">        methods called on subclasses return a subclass instance instead of</span>
<span class="sd">        a ``torch.Tensor`` instance.</span>

<span class="sd">        One corollary to this is that you need coverage for torch.Tensor</span>
<span class="sd">        methods if implementing __torch_function__ for subclasses.</span>

<span class="sd">        We recommend always calling ``super().__torch_function__`` as the base</span>
<span class="sd">        case when doing the above.</span>

<span class="sd">        While not mandatory, we recommend making `__torch_function__` a classmethod.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">types</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

        <span class="k">with</span> <span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">get_default_nowrap_functions</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">ret</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>

    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_dispatch_impl</span>

    <span class="k">def</span> <span class="nf">__dlpack__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_</span>
<span class="sd">        of the current tensor to be exported to other libraries.</span>

<span class="sd">        This function will be called from the `from_dlpack` method</span>
<span class="sd">        of the library that will consume the capsule. `from_dlpack` passes the current</span>
<span class="sd">        stream to this method as part of the specification.</span>

<span class="sd">        Args:</span>
<span class="sd">            stream (integer or None): An optional Python integer representing a</span>
<span class="sd">            pointer to a CUDA stream. The current stream is synchronized with</span>
<span class="sd">            this stream before the capsule is created, and since the capsule</span>
<span class="sd">            shares its storage with the tensor this make it safe to access from</span>
<span class="sd">            both streams.  If None or -1 is passed then no synchronization is performed.</span>
<span class="sd">            If 1 (on CUDA) or 0 (on ROCM) then the default stream is used for</span>
<span class="sd">            synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>

        <span class="c1"># DLPack capsules can&#39;t capture all of PyTorch&#39;s semantics,</span>
        <span class="c1"># so we prohibit exporting tensors that would lose their properties like</span>
        <span class="c1"># requires_grad and having the conjugate bit set.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors that require gradient, use tensor.detach()&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t export tensors with the conjugate bit set&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors with layout other than torch.strided&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
            <span class="c1"># Stream pointers in CUDA/ROCm are uniquely numbered and can</span>
            <span class="c1"># be retrieved from their integer value.</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;stream must be ``int`` or ``none``&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">stream</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="c1"># NB: This logic handles the special case values for default</span>
                <span class="c1"># streams and must be kept in sync with from_dlpack in</span>
                <span class="c1"># torch/utils/dlpack.py</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ExternalStream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
                <span class="c1"># Only synchronize on different streams</span>
                <span class="n">sync_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">!=</span> <span class="n">sync_stream</span><span class="p">:</span>
                    <span class="n">event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
                    <span class="n">event</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">sync_stream</span><span class="p">)</span>
                    <span class="n">stream</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__dlpack_device__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack_device__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">torch.utils.dlpack</span> <span class="kn">import</span> <span class="n">DLDeviceType</span>

        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">torch_device_type</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
        <span class="k">if</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLROCM</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">():</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPUPinned</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPU</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLOneAPI</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown device type </span><span class="si">{</span><span class="n">torch_device_type</span><span class="si">}</span><span class="s2"> for Dlpack&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>


<span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="c1"># Also handles things like namedtuples</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)(</span><span class="n">_convert</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright (c) 2023 Graphcore Ltd. All rights reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>